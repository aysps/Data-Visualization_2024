ungroup() %>%
mutate(bigram = fct_inorder(bigram))
ggplot(hp_bigrams, aes(x = n, y = fct_rev(bigram), fill = book)) +
geom_col() +
guides(fill = "none") +
labs(x = NULL, y = NULL) +
scale_fill_viridis_d() +
facet_wrap(vars(book), scales = "free_y") +
theme_bw()
pronouns <- c("he", "she")
bigram_he_she_counts <- hp %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)  %>%
count(bigram, sort = TRUE) %>%
# Split the bigram column into two columns
separate(bigram, c("word1", "word2"), sep = " ") %>%
# Only choose rows where the first word is he or she
filter(word1 %in% pronouns) %>%
count(word1, word2, wt = n, sort = TRUE) %>%
rename(total = n)
word_ratios <- bigram_he_she_counts %>%
# Look at each of the second words
group_by(word2) %>%
# Only choose rows where the second word appears more than 10 times
filter(sum(total) > 10) %>%
ungroup() %>%
# Spread out the word1 column so that there's a column named "he" and one named "she"
spread(word1, total, fill = 0) %>%
# Add 1 to each number so that logs work (just in case any are zero)
mutate_if(is.numeric, ~(. + 1) / sum(. + 1)) %>%
# Create a new column that is the logged ratio of the she counts to he counts
mutate(logratio = log2(she / he)) %>%
# Sort by that ratio
arrange(desc(logratio))
# Rearrange this data so it's plottable
plot_word_ratios <- word_ratios %>%
# This gets the words in the right order---we take the absolute value, select
# only rows where the log ratio is bigger than 0, and then take the top 15 words
mutate(abslogratio = abs(logratio)) %>%
group_by(logratio < 0) %>%
top_n(10, abslogratio) %>%
ungroup() %>%
mutate(word = reorder(word2, logratio))
ggplot(plot_word_ratios, aes(word, logratio, color = logratio < 0)) +
geom_segment(aes(x = word, xend = word,
y = 0, yend = logratio),
linewidth = 1.1, alpha = 0.6) +
geom_point(size = 3.5) +
coord_flip() +
labs(y = "How much more/less likely", x = NULL) +
scale_color_manual(name = "", labels = c("More 'she'", "More 'he'"),
values = c("#3D9970", "#FF851B")) +
scale_y_continuous(breaks = seq(-3, 3),
labels = c("8x", "4x", "2x",
"Same", "2x", "4x", "8x")) +
theme_bw() +
theme(legend.position = "bottom")
library(cleanNLP)
cnlp_init_udpipe()
hp_to_tag <- hp %>%
mutate(book_chapter = paste0(book, "_", chapter))
# This took like 15-20 minutes for the whole HP series (!!!)
hp_pos <- cnlp_annotate(hp_to_tag, text_name = "text", doc_name = "book_chapter")
saveRDS(hp_pos, "~/Desktop/hp_pos.rds")
saveRDS(hp_pos, "~/Desktop/hp_pos.rds")
# This took like 15-20 minutes for the whole HP series (!!!)
hp_pos <- cnlp_annotate(hp_to_tag, text_name = "text", doc_name = "book_chapter")
saveRDS(hp_pos, "~/Desktop/hp_pos.rds")
hp1_pos_terms <- hp_pos$token %>%
filter(doc_id < 18)
hp1_pos_terms %>%
head(50) %>%
write_csv("~/Desktop/hp_pos_small.csv")
hp_1_verbs <- hp1_pos_terms %>%
filter(upos == "VERB") %>%
count(lemma, sort = TRUE)
hp_1_nouns <- hp1_pos_terms %>%
filter(upos %in% c("NOUN", "PROPN")) %>%
count(lemma, sort = TRUE)
hp_1_adj_adv <- hp1_pos_terms %>%
filter(upos %in% c("ADJ", "ADV")) %>%
count(lemma, sort = TRUE)
write_csv(hp_1_verbs, "~/Desktop/hp1_verbs.csv")
write_csv(hp_1_nouns, "~/Desktop/hp1_nouns.csv")
write_csv(hp_1_adj_adv, "~/Desktop/hp1_adj_adv.csv")
write_csv(hp_1_verbs, "~/Desktop/hp1_verbs.csv")
write_csv(hp_1_nouns, "~/Desktop/hp1_nouns.csv")
write_csv(hp_1_adj_adv, "~/Desktop/hp1_adj_adv.csv")
```{r show-hp-pos, echo=FALSE}
hp_1_verbs <- read_csv("data/hp1_verbs.csv")
saveRDS(hp_pos, "data/hp_pos.rds")
hp1_pos_terms <- hp_pos$token %>%
filter(doc_id < 18)
hp1_pos_terms %>%
head(50) %>%
write_csv("data/hp_pos_small.csv")
hp_1_verbs <- hp1_pos_terms %>%
filter(upos == "VERB") %>%
count(lemma, sort = TRUE)
hp_1_nouns <- hp1_pos_terms %>%
filter(upos %in% c("NOUN", "PROPN")) %>%
count(lemma, sort = TRUE)
hp_1_adj_adv <- hp1_pos_terms %>%
filter(upos %in% c("ADJ", "ADV")) %>%
count(lemma, sort = TRUE)
write_csv(hp_1_verbs, "data/hp1_verbs.csv")
write_csv(hp_1_nouns, "data/hp1_nouns.csv")
write_csv(hp_1_adj_adv, "data/hp1_adj_adv.csv")
write_csv(hp_1_verbs, "data/hp1_verbs.csv")
write_csv(hp_1_nouns, "data/hp1_nouns.csv")
write_csv(hp_1_adj_adv, "data/hp1_adj_adv.csv")
```{r show-hp-pos, echo=FALSE}
hp_1_verbs <- read_csv("data/hp1_verbs.csv")
hp_1_nouns <- read_csv("data/hp1_nouns.csv")
hp_1_adj_adv <- read_csv("data/hp1_adj_adv.csv")
hp_1_pos_small <- read_csv("data/hp_pos_small.csv")
hp_1_pos_small <- read_csv("data/hp_pos_small.csv")
.small-code[
```{r echo=FALSE, comment=NA}
hp_1_pos_small
hp_1_verbs
# Chunk 1: setup
knitr::opts_chunk$set(warning = FALSE, message = FALSE,
fig.retina = 3, fig.align = "center")
# Chunk 2: packages-data
library(tidyverse)
library(tidytext)
# devtools::install_github("bradleyboehmke/harrypotter")
library(harrypotter)
set.seed(1234)
options("digits" = 2, "width" = 90)
hp <- tibble(book = c("Philosopher's Stone", "Chamber of Secrets",
"Prisoner of Azkaban", "Goblet of Fire",
"Order of the Phoenix", "Half-Blood Prince",
"Deathly Hallows"),
raw_text = list(philosophers_stone, chamber_of_secrets,
prisoner_of_azkaban, goblet_of_fire,
order_of_the_phoenix, half_blood_prince,
deathly_hallows)) %>%
mutate(text_data = map(raw_text, ~{
tibble(text = .x) %>%
mutate(chapter = 1:n())
})) %>%
select(book, text_data) %>%
unnest(text_data) %>%
mutate(book = fct_inorder(book))
# Chunk 3: xaringanExtra
xaringanExtra::use_xaringan_extra(c("tile_view", "share_again"))
# Chunk 4: show-hp1
harrypotter::philosophers_stone[1] %>%
str_trunc(90 * 14) %>%
str_wrap(90) %>%
cat("\n")
# Chunk 5: tidy-hp1
hp1_data <- tibble(text = harrypotter::philosophers_stone) %>%
mutate(chapter = 1:n(),
book = "Harry Potter and the Philosopher's Stone") %>%
select(chapter, book, text)
head(hp1_data)
# Chunk 6: hp1-word
hp1_words <- hp1_data %>%
unnest_tokens(word, text) %>%
mutate(book = str_trunc(book, width = 15)) %>%
select(word, chapter, book) %>%
head()
hp1_words
# Chunk 7: hp1-bigram
hp1_bigrams <- hp1_data %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
mutate(book = str_trunc(book, width = 15)) %>%
select(bigram, chapter, book) %>%
head()
hp1_bigrams
# Chunk 8: show-stop-words
stop_words
# Chunk 9: hp-words
hp_tokens <- hp %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
group_by(book) %>%
count(word, sort = TRUE) %>%
top_n(9, n) %>%
ungroup() %>%
mutate(word = fct_inorder(word))
ggplot(hp_tokens, aes(x = n, y = fct_rev(word), fill = book)) +
geom_col() +
guides(fill = "none") +
labs(x = NULL, y = NULL) +
scale_fill_viridis_d() +
facet_wrap(vars(book), scales = "free_y") +
theme_bw()
# Chunk 10: hp-bigrams
hp_bigrams <- hp %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
unite(bigram, word1, word2, sep = " ") %>%
group_by(book) %>%
count(bigram, sort = TRUE) %>%
top_n(9, n) %>%
ungroup() %>%
mutate(bigram = fct_inorder(bigram))
get_sentiments("bing")
get_sentiments("afinn")
install.packages("textdata")
get_sentiments("afinn")
get_sentiments("nrc")
# https://rstudio-pubs-static.s3.amazonaws.com/300624_8260952d1f0346969e65f41a97006bf5.html
hp_sentiment <- hp %>%
unnest_tokens(word, text) %>%
group_by(book) %>%
mutate(word_count = 1:n(),
index = word_count %/% 500 + 1) %>%
inner_join(get_sentiments("bing")) %>%
count(book, index = index, sentiment) %>%
pivot_wider(names_from = sentiment, values_from = n) %>%
mutate(net_sentiment = positive - negative)
ggplot(hp_sentiment,
aes(x = index, y = net_sentiment, fill = net_sentiment > 0)) +
geom_col() +
guides(fill = "none") +
labs(x = NULL, y = "Net sentiment") +
facet_wrap(vars(book), scales = "free_x") +
theme_bw()
ggplot(hp_sentiment,
aes(x = index, y = net_sentiment, fill = net_sentiment > 0)) +
geom_col() +
guides(fill = "none") +
labs(x = NULL, y = "Net sentiment") +
facet_wrap(vars(book), scales = "free_x") +
theme_bw()
---
# tf-idf
.box-inv-5[Term frequency-inverse document frequency]
.box-5.small[How important a term is compared to the rest of the documents]
# Get a list of words in all the books
hp_words <- hp %>%
unnest_tokens(word, text) %>%
count(book, word, sort = TRUE) %>%
ungroup()
# Add the tf-idf for these words
hp_tf_idf <- hp_words %>%
bind_tf_idf(word, book, n) %>%
arrange(desc(tf_idf))
# Get the top 10 uniquest words
hp_tf_idf_plot <- hp_tf_idf %>%
group_by(book) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = fct_inorder(word))
ggplot(hp_tf_idf_plot, aes(y = fct_rev(word), x = tf_idf, fill = book)) +
geom_col() +
guides(fill = "none") +
labs(y = "tf-idf", x = NULL) +
facet_wrap(~ book, scales = "free") +
theme_bw()
ggplot(hp_tf_idf_plot, aes(y = fct_rev(word), x = tf_idf, fill = book)) +
geom_col() +
guides(fill = "none") +
labs(y = "tf-idf", x = NULL) +
facet_wrap(~ book, scales = "free") +
theme_bw()
---
# Topic modeling
.pull-left.center[
<figure>
.pull-right.center[
<figure>
???
https://commons.wikimedia.org/wiki/File:Laurel_Thatcher_Ulrich_(32803708014).jpg
https://commons.wikimedia.org/wiki/File:A_Midwife%27s_Tale_by_Laurel_Thatcher_Ulrich.jpg
targets::tar_make()
# THE MAIN PIPELINE ----
list(
##  Run all the data building and copying targets ----
save_data,
### Link all these data building and copying targets into individual dependencies ----
tar_combine(copy_data, tar_select_targets(save_data, starts_with("copy_"))),
tar_combine(build_data, tar_select_targets(save_data, starts_with("data_"))),
## xaringan stuff ----
#
### Knit xaringan slides ----
#
# Use dynamic branching to get a list of all .Rmd files in slides/ and knit them
#
# The main index.qmd page loads xaringan_slides as a target to link it as a dependency
tar_files(xaringan_files, list.files(here_rel("slides"),
pattern = "\\.Rmd",
full.names = TRUE)),
tar_target(xaringan_slides,
render_xaringan(xaringan_files),
pattern = map(xaringan_files),
format = "file"),
### Convert xaringan HTML slides to PDF ----
#
# Use dynamic branching to get a list of all knitted slide .html files and
# convert them to PDF with pagedown
#
# The main index.qmd page loads xaringan_pdfs as a target to link it as a dependency
tar_files(xaringan_html_files, {
xaringan_slides
list.files(here_rel("slides"),
pattern = "\\.html",
full.names = TRUE)
}),
tar_target(xaringan_pdfs,
xaringan_to_pdf(xaringan_html_files),
pattern = map(xaringan_html_files),
format = "file"),
## Project folders ----
### Zip up each project folder ----
#
# Get a list of all folders in the project folder, create dynamic branches,
# then create a target for each that runs the custom zippy() function, which
# uses system2() to zip the folder and returns a path to keep targets happy
# with `format = "file"`
#
# The main index.qmd page loads project_zips as a target to link it as a dependency
#
# Use tar_force() and always run this because {targets} seems to overly cache
# the results of list.dirs()
tar_force(project_paths,
list.dirs(here_rel("projects"),
full.names = FALSE, recursive = FALSE),
force = TRUE),
tar_target(project_files, project_paths, pattern = map(project_paths)),
tar_target(project_zips, {
copy_data
build_data
zippy(project_files, "projects")
},
pattern = map(project_files),
format = "file"),
## Class schedule calendar ----
tar_target(schedule_file, here_rel("data", "schedule.csv"), format = "file"),
tar_target(schedule_page_data, build_schedule_for_page(schedule_file)),
tar_target(
schedule_ical_data,
build_ical(
schedule_file, base_url,
page_suffix, class_number
)
),
tar_target(
schedule_ical_file,
save_ical(
schedule_ical_data,
here_rel("files", "schedule.ics")
),
format = "file"
),
## Knit the README ----
# tar_target(workflow_graph, tar_mermaid(
#   targets_only = TRUE, outdated = FALSE,
#   legend = FALSE, color = FALSE, store = "_targets"
# )),
# tar_quarto(readme, here_rel("README.qmd")),
## Build site ----
tar_quarto(site, path = ".", quiet = FALSE),
## Upload site ----
tar_target(deploy_script, here_rel("deploy.sh"), format = "file"),
tar_target(deploy_site, {
# Force dependencies
site
# Run the deploy script
if (Sys.getenv("UPLOAD_WEBSITES") == "TRUE") {
processx::run(paste0("./", deploy_script))
}
})
)
library(targets)
install.packages("targets")
install.packages("tarchetypes")
library(targets)
remove.packages(targets)
remove.packages("targets")
remove.packages("tarchetypes")
install.packages("targets")
install.packages("tarchetypes")
library(targets)
library(tarchetypes)
suppressPackageStartupMessages(library(tidyverse))
class_number <- "PMAP 8551/4551"
base_url <- "https://datavizf23.classes.andrewheiss.com/"
page_suffix <- ".html"
options(
tidyverse.quiet = TRUE,
dplyr.summarise.inform = FALSE
)
tar_option_set(
packages = c("tibble"),
format = "rds",
workspace_on_error = TRUE
)
# here::here() returns an absolute path, which then gets stored in tar_meta and
# becomes computer-specific (i.e. /Users/andrew/Research/blah/thing.Rmd).
# There's no way to get a relative path directly out of here::here(), but
# fs::path_rel() works fine with it (see
# https://github.com/r-lib/here/issues/36#issuecomment-530894167)
here_rel <- function(...) {fs::path_rel(here::here(...))}
# Load functions for the pipeline
source("R/tar_calendar.R")
source("R/tar_slides.R")
source("R/tar_projects.R")
source("R/tar_data.R")
# THE MAIN PIPELINE ----
list(
##  Run all the data building and copying targets ----
save_data,
### Link all these data building and copying targets into individual dependencies ----
tar_combine(copy_data, tar_select_targets(save_data, starts_with("copy_"))),
tar_combine(build_data, tar_select_targets(save_data, starts_with("data_"))),
## xaringan stuff ----
#
### Knit xaringan slides ----
#
# Use dynamic branching to get a list of all .Rmd files in slides/ and knit them
#
# The main index.qmd page loads xaringan_slides as a target to link it as a dependency
tar_files(xaringan_files, list.files(here_rel("slides"),
pattern = "\\.Rmd",
full.names = TRUE)),
tar_target(xaringan_slides,
render_xaringan(xaringan_files),
pattern = map(xaringan_files),
format = "file"),
### Convert xaringan HTML slides to PDF ----
#
# Use dynamic branching to get a list of all knitted slide .html files and
# convert them to PDF with pagedown
#
# The main index.qmd page loads xaringan_pdfs as a target to link it as a dependency
tar_files(xaringan_html_files, {
xaringan_slides
list.files(here_rel("slides"),
pattern = "\\.html",
full.names = TRUE)
}),
tar_target(xaringan_pdfs,
xaringan_to_pdf(xaringan_html_files),
pattern = map(xaringan_html_files),
format = "file"),
## Project folders ----
### Zip up each project folder ----
#
# Get a list of all folders in the project folder, create dynamic branches,
# then create a target for each that runs the custom zippy() function, which
# uses system2() to zip the folder and returns a path to keep targets happy
# with `format = "file"`
#
# The main index.qmd page loads project_zips as a target to link it as a dependency
#
# Use tar_force() and always run this because {targets} seems to overly cache
# the results of list.dirs()
tar_force(project_paths,
list.dirs(here_rel("projects"),
full.names = FALSE, recursive = FALSE),
force = TRUE),
tar_target(project_files, project_paths, pattern = map(project_paths)),
tar_target(project_zips, {
copy_data
build_data
zippy(project_files, "projects")
},
pattern = map(project_files),
format = "file"),
## Class schedule calendar ----
tar_target(schedule_file, here_rel("data", "schedule.csv"), format = "file"),
tar_target(schedule_page_data, build_schedule_for_page(schedule_file)),
tar_target(
schedule_ical_data,
build_ical(
schedule_file, base_url,
page_suffix, class_number
)
),
tar_target(
schedule_ical_file,
save_ical(
schedule_ical_data,
here_rel("files", "schedule.ics")
),
format = "file"
),
## Knit the README ----
# tar_target(workflow_graph, tar_mermaid(
#   targets_only = TRUE, outdated = FALSE,
#   legend = FALSE, color = FALSE, store = "_targets"
# )),
# tar_quarto(readme, here_rel("README.qmd")),
## Build site ----
tar_quarto(site, path = ".", quiet = FALSE),
## Upload site ----
tar_target(deploy_script, here_rel("deploy.sh"), format = "file"),
tar_target(deploy_site, {
# Force dependencies
site
# Run the deploy script
if (Sys.getenv("UPLOAD_WEBSITES") == "TRUE") {
processx::run(paste0("./", deploy_script))
}
})
)
targets::tar_make()
system2("cd", c(parent, "; zip", "-rX", paste0(folder_to_zip, ".zip"),
folder_to_zip, '-x "*.DS_Store" "**/.Rproj.user/*"'))
system2("cd", c(parent, "; zip", "-rX", paste0(projects, ".zip"),
folder_to_zip, '-x "*.DS_Store" "**/.Rproj.user/*"'))
system2("cd", c(parent, "; zip", "-rX", paste0(projects, ".zip"),
projects, '-x "*.DS_Store" "**/.Rproj.user/*"'))
return(file.path(C:/Users/Eun Joo Kwon/Downloads/datavizf23.classes.andrewheiss.com-main, paste0(projects, ".zip")))
?zippy
