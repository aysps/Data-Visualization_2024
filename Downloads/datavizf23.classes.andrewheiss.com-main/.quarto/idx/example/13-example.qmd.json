{"title":"Text","markdown":{"yaml":{"title":"Text","date":"2023-11-13","date_end":"2023-11-17"},"headingText":"Live coding example","containsRefs":false,"markdown":"\n\n```{r load-targets, include=FALSE}\nwithr::with_dir(here::here(), {\n  little_women_path <- targets::tar_read(data_little_women_tagged)\n  little_women_tagged <- targets::tar_read(little_women_tagged)$spacy$token\n})\n```\n\nFor this example, we're going to use the text of *Little Women* by Louisa May Alcott and four Shakespearean tragedies (*Romeo and Juliet*, *King Lear*, *Macbeth*, and *Hamlet*) to explore how to do some basic text visualization.\n\nYou can follow along if you want, but **don't feel like you have too**. This is mostly just to give you a taste of different methods for visualizing text. It's by no means comprehensive, but it is well annotated and commented and should (hopefully) be easy to follow.\n\nIf you want to play with part-of-speech tagging, you can download an already-tagged version of *Little Women* here (you'll likely need to right click and choose \"Save Link As…\"):\n\n- [{{< fa file-csv >}} `little_women_tagged.csv`](/`r little_women_path`)\n\nIf you want to see other examples of text visualizations with the {tidytext} package, check out some of these:\n\n- {{< fa arrow-up-right-from-square >}} [Harry Potter Sentiment Analysis for Beginners](https://rstudio-pubs-static.s3.amazonaws.com/300624_8260952d1f0346969e65f41a97006bf5.html) (this uses [the {harrypotter} package](https://github.com/bradleyboehmke/harrypotter), which you can install from GitHub (not from CRAN))\n- {{< fa arrow-up-right-from-square >}} Peer Christensen [\"Fair is foul, and foul is fair: a tidytext sentiment analysis of Shakespeare’s tragedies\"](https://peerchristensen.netlify.app/post/fair-is-foul-and-foul-is-fair-a-tidytext-entiment-analysis-of-shakespeare-s-tragedies/)\n- {{< fa arrow-up-right-from-square >}} [\"Tidy text, parts of speech, and unique words in the Bible\"](https://www.andrewheiss.com/blog/2018/12/26/tidytext-pos-john/)\n- {{< fa arrow-up-right-from-square >}} [\"Tidy text, parts of speech, and unique words in the Qur'an\"](https://www.andrewheiss.com/blog/2018/12/28/tidytext-pos-arabic/)\n\n\n\n<div class=\"ratio ratio-16x9\">\n<iframe src=\"https://www.youtube.com/embed/YeyZp8Dw55g\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\" frameborder=\"0\"></iframe>\n</div>\n\n\n::: {.callout-important}\n### Big differences from the video\n\nThis is a highly cleaned up version of the code from the video.\n:::\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(fig.width = 7, fig.height = 4.2, fig.align = \"center\", collapse = TRUE)\nset.seed(1234)\noptions(\"digits\" = 2, \"width\" = 150)\noptions(dplyr.summarise.inform = FALSE)\n```\n\n## Get data\n\nFirst, as always, we'll load the libraries we'll be using:\n\n```{r load-libraries, warning=FALSE, message=FALSE}\nlibrary(tidyverse)   # For ggplot, dplyr, etc.\nlibrary(tidytext)    # For neat text things\nlibrary(gutenbergr)  # For downloading books from Project Gutenberg\n```\n\nWe're going to use the {gutenbergr} package to download some books directly from Project Gutenberg. The IDs for these books come from the URLs at their website. For instance, [*Little Women* is book #514](https://www.gutenberg.org/ebooks/514). We'll store these books as `*_raw` and then clean them up later.\n\n```{r get-text-fake, eval=FALSE}\n# 514 Little Women\nlittle_women_raw <- gutenberg_download(514, meta_fields = \"title\")\n\n# 1524 - Hamlet\n# 1532 - King Lear\n# 1533 - Macbeth\n# 1513 - Romeo and Juliet\ntragedies_raw <- gutenberg_download(c(1524, 1532, 1533, 1513),\n                                    meta_fields = \"title\")\n```\n\n```{r load-saved-text, include=FALSE}\nwithr::with_dir(here::here(), {\n  invisible(list2env(targets::tar_read(gutenberg_books), .GlobalEnv))\n})\n```\n\n::: {.callout-important}\n### Downloading errors\n\nSometimes the Project Gutenberg server gets too much traffic and goes down temporarily (it's all run by volunteers!) and you'll get an error like this:\n\n```default\nWarning: Could not download a book at http://aleph.gutenberg.org/...\n```\n\nProject Gutenberg has multiple copies of itself on different servers around the world, called \"mirrors.\" You can see [the full list of mirrors here](https://www.gutenberg.org/MIRRORS.ALL). If you get an error about connecting to the main Project Gutenberg server (aleph.gutenberg.org), you can specify a different mirror with the `mirror` argument in `gutenberg_download()`:\n\n```{r gutenberg-mirror, eval=FALSE}\nlittle_women_raw <- gutenberg_download(514, meta_fields = \"title\",\n                                       mirror = \"https://gutenberg.pglaf.org/\")\n```\n:::\n\nIf you won't want to redownload the books every time you knit (you don't), you can do the same trick we've used for [WDI](/example/08-example.qmd) and [FRED data](/example/11-example.qmd). Put the actual code for getting the books in a chunk with `eval=FALSE` on it and run it manually in RStudio when you want to get the data. Then you can write the downloaded data as a CSV file, and then load it invisibly from the CSV file when you knit:\n\n````\nI first download data from Project Gutenberg:\n\n```{{r get-book, eval=FALSE}}\nbooks_raw <- gutenberg_download(...)\n\nwrite_csv(books_raw, \"data/books_raw.csv\")\n```\n\n```{{r load-book-data-real, include=FALSE}}\nbooks_raw <- read_csv(\"data/books_raw.csv\")\n```\n\n````\n\n## Clean data\n\nThe data you get from Project Gutenberg comes in a tidy format, with a column for the book id, a column for the title, and a column for text. Sometimes this text column will be divided by lines in the book; sometimes it might be an entire page or paragraph or chapter. It all depends on how the book is formatted at Project Gutenberg.\n\nHere's what the start of our `little_women_raw` data looks like:\n\n```{r show-lw-start}\nhead(little_women_raw)\n```\n\nIf we look at the data in RStudio, we can see that the actual book doesn't start until row 70 (the first 69 rows are the table of contents and other parts of the front matter).\n\n::: {.callout-note}\n### Every book is different!\n\nIn this case, *Little Women* starts at row 67. **That will not be true for all books!** Every book is unique and has different amounts of front matter. You cannot assume that any book you work with starts at line 67.\n:::\n\nIt would be nice if we had a column that indicated what chapter each line is in, since we could then group by chapter and look at patterns within chapters. Since the data doesn't come with a chapter column, we have to make one ourselves using a fun little trick. Each chapter in the book starts with \"CHAPTER ONE\" or \"CHAPTER TWO\", with \"chapter\" in ALL CAPS. We can make a variable named `chapter_start` that will be true if a line starts with \"CHAPTER\" and false if not. Then we can use the `cumsum()` function to take the cumulative sum of this column, which will increment up one number ever time there's a new chapter, thus creating a helpful chapter column.\n\n```{r clean-data-lw}\n# Clean up Little Women\nlittle_women <- little_women_raw %>% \n  # The actual book doesn't start until line 67\n  slice(67:n()) %>% \n  # Get rid of rows where text is missing\n  drop_na(text) %>% \n  # Chapters start with CHAPTER X, so mark if each row is a chapter start\n  # cumsum() calculates the cumulative sum, so it'll increase every time there's\n  # a new chapter and automatically make chapter numbers\n  mutate(chapter_start = str_detect(text, \"^CHAPTER\"),\n         chapter_number = cumsum(chapter_start)) %>% \n  # Get rid of these columns\n  select(-gutenberg_id, -title, -chapter_start)\n\nhead(little_women)\n```\n\nThe data from Shakespeare is similarly messy, with just three columns:\n\n```{r show-shakespeare-raw}\nhead(tragedies_raw)\n```\n\nThe initial text sometimes isn't the actual text of the book. If you look at the beginning of *Hamlet*, for instance, there's a bunch of introductory stuff from editors and transcribers. In real life, we'd want to find a systematic way to get rid of that (perhaps by looking at how many introductory rows there are in each of the four plays and removing those rows), but for now, we'll just live with it and pretend Shakespeare wrote these notes. `r emoji::emoji(\"shrug\")`\n\nWe could also figure out a systematic way to indicate acts and scenes, but that's tricky, so we won't for this example. ([This guy did though!](https://peerchristensen.netlify.app/post/fair-is-foul-and-foul-is-fair-a-tidytext-entiment-analysis-of-shakespeare-s-tragedies/))\n\nNow that we have tidy text data, let's do stuff with it!\n\n\n## Tokens and word counts\n\n### Single words\n\nOne way we can visualize text is to look at word frequencies and find the most common words. This is even more important when looking across documents. \n\nRight now the text we have is tidy, but it is based on lines of text, not words. In order to count words correctly, we need each token (or text element, whether it be a word or bigram or paragraph or whatever) to be in its own row. The `unnest_tokens()` functions from {tidytext} does this for us. The first argument is the name of the column we want to create; the second argument is the name of the column we want to split into tokens.\n\nLet's just work with the Shakespeare tragedies:\n\n```{r tidy-shakespeare-1}\ntragedies_words <- tragedies_raw %>% \n  drop_na(text) %>% \n  unnest_tokens(word, text)\n\nhead(tragedies_words)\n```\n\nNow that we have words, we can filter and count the words. Here's what's happening in this next chunk:\n\n- We use `anti_join()` to remove all common stop words like \"a\" and \"the\" that are listed in the `stop_words` dataset that is loaded when you load {tidytext}\n- We count how many times each word appears in each title/play\n- We only keep the top 15 words\n\n```{r top-15-shakespeare, message=FALSE, warning=FALSE}\ntop_words_tragedies <- tragedies_words %>% \n  # Remove stop words\n  anti_join(stop_words) %>% \n  # Get rid of old timey words and stage directions\n  filter(!(word %in% c(\"thou\", \"thy\", \"haue\", \"thee\", \n                      \"thine\", \"enter\", \"exeunt\", \"exit\"))) %>% \n  # Count all the words in each play\n  count(title, word, sort = TRUE) %>% \n  # Keep top 15 in each play\n  group_by(title) %>% \n  top_n(15) %>% \n  ungroup() %>% \n  # Make the words an ordered factor so they plot in order\n  mutate(word = fct_inorder(word))\ntop_words_tragedies\n```\n\nNow we can plot these results, facetting and filling by title:\n\n```{r plot-top-words-tragedies}\nggplot(top_words_tragedies, aes(y = fct_rev(word), x = n, fill = title)) + \n  geom_col() + \n  guides(fill = \"none\") +\n  labs(y = \"Count\", x = NULL, \n       title = \"15 most frequent words in four Shakespearean tragedies\") +\n  facet_wrap(vars(title), scales = \"free_y\") +\n  theme_bw()\n```\n\nThese results aren't terribly surprising. \"lear\" is the most common word in *King Lear*, \"macbeth\" is the most common word in *Macbeth*, and so on. But the results are still really neat! This is a wordcloud for grownups!\n\n(Sharp-eyed readers will notice that the words aren't actually in perfect order! That's because some common words are repeated across the plays, like \"lord\" and \"sir\". However, each category in a factor can only have one possible position in the orer, so because \"lord\" is the second most common word in *Hamlet* it also appears as #2 in *Macbeth* and *King Lear*. You can fix this with the `reorder_within()` function in {tidytext}—see [Julia Silge's tutorial here](https://juliasilge.com/blog/reorder-within/) for how to use it.)\n\n### Bigrams\n\nWe can also look at pairs of words instead of single words. To do this, we need to change a couple arguments in `unnest_tokens()`, but otherwise everything else stays the same. In order to remove stopwords, we need to split the bigram column into two columns (`word1` and `word2`) with `separate()`, filter each of those columns, and then combine the word columns back together as `bigram` with `unite()`\n\n```{r top-bigrams}\ntragedies_bigrams <- tragedies_raw %>% \n  drop_na(text) %>% \n  # n = 2 here means bigrams. We could also make trigrams (n = 3) or any type of n-gram\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) %>% \n  # Get rid of NAs in the new bigram column\n  drop_na(bigram) %>% \n  # Split the bigrams into two words so we can remove stopwords\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>% \n  filter(!word1 %in% stop_words$word,\n         !word2 %in% stop_words$word) %>% \n  filter(!word1 %in% c(\"thou\", \"thy\", \"thine\", \"enter\", \"exeunt\", \"exit\"),\n         !word2 %in% c(\"thou\", \"thy\", \"thine\", \"enter\", \"exeunt\", \"exit\")) %>% \n  # Put the two word columns back together\n  unite(bigram, word1, word2, sep = \" \")\ntragedies_bigrams\n\ntop_bigrams <- tragedies_bigrams %>% \n  # Count all the bigrams in each play\n  count(title, bigram, sort = TRUE) %>% \n  # Keep top 15 in each play\n  group_by(title) %>% \n  top_n(15) %>% \n  ungroup() %>% \n  # Make the bigrams an ordered factor so they plot in order\n  mutate(bigram = fct_inorder(bigram))\n\nggplot(top_bigrams, aes(y = fct_rev(bigram), x = n, fill = title)) + \n  geom_col() + \n  guides(fill = \"none\") +\n  labs(y = \"Count\", x = NULL, \n       title = \"15 most frequent bigrams in four Shakespearean tragedies\") +\n  facet_wrap(vars(title), scales = \"free\") +\n  theme_bw()\n```\n\nThere are some neat trends here. \"Lord Hamlet\" is the most common pair of words in *Hamlet* (not surprisingly), but in Macbeth the repeated \"knock knock\" (the first non-name repeated pair) is a well-known plot point and reoccurring symbolic theme throughout the play.\n\n## Bigrams and probability\n\nWe can replicate the [\"She Giggles, He Gallops\"](https://pudding.cool/2017/08/screen-direction/) idea by counting the bigrams that match \"he X\" and \"she X\". \n\nThe log ratio idea shows how much more likely a word is compared to its counterpart (so \"he that\" is about 5 more likely to appear than \"she that\". In this graph, I replaced the x-axis labels with \"2x\" and \"4x\", but without those, you get numbers like 1, 2, and 3 (or -1, -2, -3)). To convert those logged ratio numbers into the multiplicative version (i.e. 2x instead of 1), raise 2 to the power of the log ratio. If the log ratio is 3, the human-readable version is $2^3$, or 8 times.\n\n```{r example-logs}\n# Take the log of 8:\nlog2(8)\n\n# Reverse log of 3:\n2^3\n```\n\nThe only text wizardry here is tokenizing the words. Pretty much the rest of all this code is just {dplyr} mutating, filtering, and counting:\n\n```{r bigrams-he-she}\npronouns <- c(\"he\", \"she\")\n\nbigram_he_she_counts <- tragedies_raw %>%\n  drop_na(text) %>% \n  # Split into bigrams\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) %>%\n  # Find counts of bigrams\n  count(bigram, sort = TRUE) %>%\n  # Split the bigram column into two columns\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%\n  # Only choose rows where the first word is he or she\n  filter(word1 %in% pronouns) %>%\n  count(word1, word2, wt = n, sort = TRUE) %>% \n  rename(total = n)\n\nword_ratios <- bigram_he_she_counts %>%\n  # Look at each of the second words\n  group_by(word2) %>%\n  # Only choose rows where the second word appears more than 10 times\n  filter(sum(total) > 10) %>%\n  ungroup() %>%\n  # Spread out the word1 column so that there's a column named \"he\" and one named \"she\"\n  spread(word1, total, fill = 0) %>%\n  # Add 1 to each number so that logs work (just in case any are zero)\n  mutate_if(is.numeric, ~(. + 1) / sum(. + 1)) %>%\n  # Create a new column that is the logged ratio of the she counts to he counts\n  mutate(logratio = log2(she / he)) %>%\n  # Sort by that ratio\n  arrange(desc(logratio))\n\n# Rearrange this data so it's plottable\nplot_word_ratios <- word_ratios %>%\n  # This gets the words in the right order---we take the absolute value, select\n  # only rows where the log ratio is bigger than 0, and then take the top 15 words\n  mutate(abslogratio = abs(logratio)) %>%\n  group_by(logratio < 0) %>%\n  top_n(15, abslogratio) %>%\n  ungroup() %>%\n  mutate(word = reorder(word2, logratio)) \n\n# Finally we plot this\nggplot(plot_word_ratios, aes(y = word, x = logratio, color = logratio < 0)) +\n  geom_segment(aes(y = word, yend = word,\n                   x = 0, xend = logratio), \n               linewidth = 1.1, alpha = 0.6) +\n  geom_point(size = 3.5) +\n  labs(x = \"How much more/less likely\", y = NULL) +\n  scale_color_discrete(name = \"\", labels = c(\"More 'she'\", \"More 'he'\")) +\n  scale_x_continuous(breaks = seq(-3, 3),\n                     labels = c(\"8x\", \"4x\", \"2x\",\n                                \"Same\", \"2x\", \"4x\", \"8x\")) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n```\n\nShakespeare doesn't use a lot of fancy verbs in his plays, so we're left with incredibly common verbs like \"should\" and \"comes\" and \"was\". Oh well.\n\n\n## Term frequency-inverse document frequency (tf-idf)\n\nWe can determine which words are the most unique for each book/document in our corpus using by calculating the tf-idf (term frequency-inverse document frequency) score for each term. The tf-idf is the product of the term frequency and the inverse document frequency:\n\n$$\n\\begin{aligned}\n\\operatorname{tf}(\\text{term}) &= \\frac{n_{\\text{term}}}{n_{\\text{terms in document}}} \\\\\n\\operatorname{idf}(\\text{term}) &= \\ln{\\left(\\frac{n_{\\text{documents}}}{n_{\\text{documents containing term}}}\\right)} \\\\\n\\operatorname{tf-idf}(\\text{term}) &= \\operatorname{tf}(\\text{term}) \\times \\operatorname{idf}(\\text{term})\n\\end{aligned}\n$$\n\nFortunately you don't need to remember that formula. The `bind_tf_idf()` function will calculate this for you. Remember, the higher the tf-idf number, the more unique the term is in the document, but these numbers are meaningless and unitless—you can't convert them to a percentage or anything.\n\nHere are the most unique words in these four tragedies, compared to all the tragedies:\n\n```{r tf-idf-tragedies, message=FALSE}\ntragedy_words <- tragedies_raw %>% \n  drop_na() %>% \n  # Split into word tokens\n  unnest_tokens(word, text) %>% \n  # Remove stop words and old timey words\n  anti_join(stop_words) %>% \n  filter(!word %in% c(\"thou\", \"thy\", \"haue\", \"thee\", \n                      \"thine\", \"enter\", \"exeunt\", \"exit\")) %>% \n  count(title, word, sort = TRUE)\n\n# Add the tf-idf values to the counts\ntragedy_tf_idf <- tragedy_words %>% \n  bind_tf_idf(word, title, n)\n\n# Get the top 10 uniquest words\ntragedy_tf_idf_plot <- tragedy_tf_idf %>% \n  arrange(desc(tf_idf)) %>% \n  group_by(title) %>% \n  top_n(10) %>% \n  ungroup() %>% \n  mutate(word = fct_inorder(word))\n\nggplot(tragedy_tf_idf_plot, \n       aes(y = fct_rev(word), x = tf_idf, fill = title)) +\n  geom_col() +\n  guides(fill = \"none\") +\n  labs(x = \"tf-idf\", y = NULL) +\n  facet_wrap(~ title, scales = \"free\") +\n  theme_bw()\n```\n\nNot surprisingly, the most unique words for each play happen to be the names of the characters in those plays.\n\n\n## Sentiment analysis\n\nIn the video, I plotted the sentiment of *Little Women* across the book, but it wasn't a very interesting plot. We'll try with Shakespeare here instead. \n\nAt its core, sentiment analysis involves looking at a big list of words for how negative or positive they are. Some sentiment dictionaries mark if a word is \"negative\" or \"positive\"; some give words a score from -3 to 3; some give different emotions like \"sadness\" or \"anger\". You can see what the different dictionaries look like with `get_sentiments()`\n\n```{r show-dictionary}\nget_sentiments(\"afinn\")  # Scoring system\n# get_sentiments(\"bing\")  # Negative/positive\n# get_sentiments(\"nrc\")  # Specific emotions\n# get_sentiments(\"loughran\")  # Designed for financial statements; positive/negative\n```\n\nHere we split the Shakespearean tragedies into words, join a sentiment dictionary to it, and use {dplyr} data wrangling to calculate the net number positive words in each chapter. Had we used the AFINN library, we could calculate the average sentiment per chapter, since AFINN uses a scoring system instead of negative/positive labels. Or we could've used the NRC library, which has specific emotions like trust and fear.\n\n```{r shakespeare-sentiment, message=FALSE}\ntragedy_words <- tragedies_raw %>% \n  drop_na() %>% \n  # Split into word tokens\n  unnest_tokens(word, text) %>% \n  # Remove stop words and old timey words\n  anti_join(stop_words) %>% \n  filter(!word %in% c(\"thou\", \"thy\", \"haue\", \"thee\", \n                      \"thine\", \"enter\", \"exeunt\", \"exit\"))\n\n# Join the sentiment dictionary \ntragedy_sentiment <- tragedy_words %>% \n  inner_join(get_sentiments(\"bing\"), relationship = \"many-to-many\")\ntragedy_sentiment\n```\n\nWe can look at these sentiments a few different ways. First we can get a count of total positive and negative words in the four books. We can see that in all four, there are more negative words than positive ones (they're tragdies, after all):\n\n```{r shakespeare-sentiment-plot-bars, message=FALSE, warning=FALSE}\ntragedy_sentiment_plot <- tragedy_sentiment %>% \n  count(title, sentiment)\n\nggplot(tragedy_sentiment_plot, aes(x = sentiment, y = n, fill = title, alpha = sentiment)) +\n  geom_col(position = position_dodge()) +\n  scale_alpha_manual(values = c(0.5, 1)) +\n  facet_wrap(vars(title)) +\n  theme_bw()\n```\n\nPerhaps more usefully, we can divide each of the plays into groups of 100 lines, and then get the net sentiment of each group (number of positive words − number of negative words). By splitting the data into groups of lines, we can show a more granular view of the progression of the plot. To do this we make a column that indicates the row number, and then we use the special `%/%` operator to perform integer division, which essentially lops off the decimal point when dividing numbers: 150/100 normally is 1.5, but in integer divison, it is 1. This is a helpful trick for putting rows 1-99 in one group, then rows 100-199 in another group, etc.\n\n```{r shakespeare-sentiment-plot-lines, message=FALSE}\ntragedies_split_into_lines <- tragedy_sentiment %>% \n  # Divide lines into groups of 100\n  mutate(line = row_number(),\n         line_chunk = line %/% 100) %>% \n  # Get a count of postiive and negative words in each 100-line chunk in each play\n  count(title, line_chunk, sentiment) %>% \n  # Convert the sentiment column into two columns named \"positive\" and \"negative\"\n  pivot_wider(names_from = sentiment, values_from = n) %>% \n  # Calculate net sentiment\n  mutate(sentiment = positive - negative)\n\nggplot(tragedies_split_into_lines,\n       aes(x = line_chunk, y = sentiment, fill = sentiment)) +\n  geom_col() +\n  scale_fill_viridis_c(option = \"magma\", end = 0.9) +\n  facet_wrap(vars(title), scales = \"free_x\") +\n  theme_bw()\n```\n\nNeat. They're all really sad and negative, except for the beginning of Romeo and Juliet where the two lovers meet and fall in love. Then everyone dies later.\n\n\n## Neat extra stuff\n\nNone of this stuff was in the video, but it's useful to know and see how to do it. It all generally comes from the [*Tidy Text Mining* book](https://www.tidytextmining.com/) by Julia Silge and David Robinson\n\n### Part of speech tagging\n\nR has no way of knowing if words are nouns, verbs, or adjectives. You can algorithmically predict what part of speech each word is using a part-of-speech tagger, like [spaCy](https://spacy.io/) or [Stanford's Natural Langauge Processing (NLP) library](https://nlp.stanford.edu/). \n\nThese are external programs that are not written in R and don't naturally communicate with R (spaCy is written in Python; Stanford's CoreNLP is written in Java). There is a helpful R package named {cleanNLP} that helps you interact with these programs from within R, whis is super helpful. {cleanNLP} also comes with its own R-only tagger so you don't need to install anything with Python or Java (however, it's not as powerful as either spaCy, which is faster, and doesn't deal with foreign languages like Arabic and Chinese like Stanford's NLP library).\n\nYou can see other examples of part-of-speech tagging (along with instructions for how to install spaCy and coreNLP) here:\n\n- {{< fa arrow-up-right-from-square >}} [\"Tidy text, parts of speech, and unique words in the Bible\"](https://www.andrewheiss.com/blog/2018/12/26/tidytext-pos-john/)\n- {{< fa arrow-up-right-from-square >}} [\"Tidy text, parts of speech, and unique words in the Qur'an\"](https://www.andrewheiss.com/blog/2018/12/28/tidytext-pos-arabic/)\n\nHere's the general process for tagging (or \"annotating\") text with the {cleanNLP} package:\n\n1. Make a dataset where one column is the id (line number, chapter number, book+chapter, etc.), and another column is the text itself.\n\n2. Initialize the NLP tagger. You can use any of these:\n\n    - `cnlp_init_udpipe()`: Use an R-only tagger that should work without installing anything extra (a little slower than the others, but requires no extra steps!)\n    - `cnlp_init_spacy()`: Use spaCy (if you've installed it on your computer with Python)\n    - `cnlp_init_corenlp()`: Use Stanford's NLP library (if you've installed it on your computer with Java) \n\n3. Feed the data frame from step 1 into the `cnlp_annotate()` function and wait.\n\n4. Save the tagged data on your computer so you don't have to re-tag it every time.\n\nHere's an example using the *Little Women* data:\n\n```{r lw-reshape}\n# For the tagger to work, each row needs to be unique, which means we need to\n# combine all the text into individual chapter-based rows. This takes a little\n# bit of text-wrangling with dplyr:\nlittle_women_to_tag <- little_women %>% \n  # Group by chapter number\n  group_by(chapter_number) %>% \n  # Take all the rows in each chapter and collapse them into a single cell\n  nest(data = c(text)) %>% \n  ungroup() %>% \n  # Look at each individual cell full of text lines and paste them together into\n  # one really long string of text per chapter\n  mutate(text = map_chr(data, ~paste(.$text, collapse = \" \"))) %>% \n  # Get rid of this column\n  select(-data)\nlittle_women_to_tag\n```\n\nNotice how there's now a row for each chapter, and the whole chapter is contained in the `text` column. With the data in this format, we can annotate it. It takes 75 seconds to run this on my 2021 MacBook Pro with the R-only udpipe tagger (and only 30 seconds if I use the spaCy tagger). Notice how I immediately save the tagged tokens as a CSV file after so I don't have to do it again. \n\n```{r nlp-example-tag, eval=FALSE}\nlibrary(cleanNLP)\n\n# Use the built-in R-based tagger\ncnlp_init_udpipe()\n\nlittle_women_tagged_raw <- cnlp_annotate(little_women_to_tag, \n  text_name = \"text\", \n  doc_name = \"chapter_number\")\n\n# Save the tagged token dataframe so we don't have to run this again\nwrite_csv(little_women_tagged_raw$token, \"data/little_women_tagged.csv\")\n\n# Load the tagged tokens\nlittle_women_tagged <- read_csv(\"data/little_women_tagged.csv\")\n```\n\nHere's what the tagged text looks like:\n\n```{r show-tagged-lw}\nlittle_women_tagged\n```\n\nThere are a bunch of new columns like `lemma` (or the base stemmed word), and `upos` and `pos` for the different parts of speech. These use the [Penn Treebank codes](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n\nNow that everything is tagged, we can do any grouping and summarizing and filtering we want. We could find the most common verbs, or the most common nouns or proper names, for instance. Here's a fun plot that shows the proportion of mentions of the four main characters (Meg, Jo, Beth, and Amy) in each chapter.\n\n```{r lw-props}\n# Find all proper nouns\nproper_nouns <- little_women_tagged %>% \n  filter(upos == \"PROPN\")\n\nmain_characters_by_chapter <- proper_nouns %>% \n  # Find only Meg, Jo, Beth, and Amy\n  filter(lemma %in% c(\"Meg\", \"Jo\", \"Beth\", \"Amy\")) %>% \n  # Group by chapter and character name\n  group_by(doc_id, lemma) %>% \n  # Get the count of mentions\n  summarize(n = n()) %>% \n  # Make a new column named \"name\" that is an ordered factor of the girls' names\n  mutate(name = factor(lemma, levels = c(\"Meg\", \"Jo\", \"Beth\", \"Amy\"), ordered = TRUE)) %>% \n  # Rename this so it's called chapter\n  rename(chapter = doc_id) %>% \n  # Group by chapter\n  group_by(chapter) %>% \n  # Calculate the proportion of each girl's mentions in each chapter\n  mutate(prop = n / sum(n)) %>% \n  ungroup() %>% \n  # Make a cleaner chapter name column\n  mutate(chapter_name = paste(\"Chapter\", chapter)) %>% \n  mutate(chapter_name = fct_inorder(chapter_name))\nmain_characters_by_chapter\n```\n\nAnd here's the polished plot:\n\n```{r lw-props-plot, fig.width=8, fig.height=5}\nggplot(main_characters_by_chapter, aes(x = prop, y = \"1\", fill = fct_rev(name))) + \n  geom_col(position = position_stack()) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.9, name = NULL) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(x = NULL, y = NULL,\n       title = \"Proportion of mentions of each\\nLittle Woman per chapter\",\n       subtitle = \"Jo basically dominates the last third of the book\") +\n  facet_wrap(vars(chapter_name), nrow = 6) +\n  theme_bw(base_family = \"Roboto Condensed\") +\n  theme(legend.position = \"top\",\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        strip.background = element_rect(fill = \"white\"),\n        legend.text = element_text(face = \"bold\", size = rel(1)),\n        plot.title = element_text(face = \"bold\", hjust = 0.5, size = rel(1.7)),\n        plot.subtitle = element_text(hjust = 0.5, size = rel(1.1)))\n```\n\n\n### Topic modeling and fingerprinting\n\nIf you want to see some examples of topic modeling with Latent Dirichlet Allocation (LDA) or text fingerprinting based on sentence length and counts of hapax legomena ([based on this article](https://kops.uni-konstanz.de/bitstream/handle/123456789/5492/Literature_Fingerprinting.pdf)), see these examples from a previous version of this class: [topic modeling](https://datavizf18.classes.andrewheiss.com/class/11-class/#topic-modeling) and [fingerprinting](https://datavizf18.classes.andrewheiss.com/class/11-class/#fingerprinting).\n\n\n### Text features\n\nFinally, you can use [the {textfeatures} package](https://github.com/mkearney/textfeatures) to find all sorts of interesting numeric statistics about text, like the number of exclamation points, commas, digits, characters per word, uppercase letters, lowercase letters, and more!\n","srcMarkdownNoYaml":"\n\n```{r load-targets, include=FALSE}\nwithr::with_dir(here::here(), {\n  little_women_path <- targets::tar_read(data_little_women_tagged)\n  little_women_tagged <- targets::tar_read(little_women_tagged)$spacy$token\n})\n```\n\nFor this example, we're going to use the text of *Little Women* by Louisa May Alcott and four Shakespearean tragedies (*Romeo and Juliet*, *King Lear*, *Macbeth*, and *Hamlet*) to explore how to do some basic text visualization.\n\nYou can follow along if you want, but **don't feel like you have too**. This is mostly just to give you a taste of different methods for visualizing text. It's by no means comprehensive, but it is well annotated and commented and should (hopefully) be easy to follow.\n\nIf you want to play with part-of-speech tagging, you can download an already-tagged version of *Little Women* here (you'll likely need to right click and choose \"Save Link As…\"):\n\n- [{{< fa file-csv >}} `little_women_tagged.csv`](/`r little_women_path`)\n\nIf you want to see other examples of text visualizations with the {tidytext} package, check out some of these:\n\n- {{< fa arrow-up-right-from-square >}} [Harry Potter Sentiment Analysis for Beginners](https://rstudio-pubs-static.s3.amazonaws.com/300624_8260952d1f0346969e65f41a97006bf5.html) (this uses [the {harrypotter} package](https://github.com/bradleyboehmke/harrypotter), which you can install from GitHub (not from CRAN))\n- {{< fa arrow-up-right-from-square >}} Peer Christensen [\"Fair is foul, and foul is fair: a tidytext sentiment analysis of Shakespeare’s tragedies\"](https://peerchristensen.netlify.app/post/fair-is-foul-and-foul-is-fair-a-tidytext-entiment-analysis-of-shakespeare-s-tragedies/)\n- {{< fa arrow-up-right-from-square >}} [\"Tidy text, parts of speech, and unique words in the Bible\"](https://www.andrewheiss.com/blog/2018/12/26/tidytext-pos-john/)\n- {{< fa arrow-up-right-from-square >}} [\"Tidy text, parts of speech, and unique words in the Qur'an\"](https://www.andrewheiss.com/blog/2018/12/28/tidytext-pos-arabic/)\n\n\n## Live coding example\n\n<div class=\"ratio ratio-16x9\">\n<iframe src=\"https://www.youtube.com/embed/YeyZp8Dw55g\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\" frameborder=\"0\"></iframe>\n</div>\n\n\n::: {.callout-important}\n### Big differences from the video\n\nThis is a highly cleaned up version of the code from the video.\n:::\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(fig.width = 7, fig.height = 4.2, fig.align = \"center\", collapse = TRUE)\nset.seed(1234)\noptions(\"digits\" = 2, \"width\" = 150)\noptions(dplyr.summarise.inform = FALSE)\n```\n\n## Get data\n\nFirst, as always, we'll load the libraries we'll be using:\n\n```{r load-libraries, warning=FALSE, message=FALSE}\nlibrary(tidyverse)   # For ggplot, dplyr, etc.\nlibrary(tidytext)    # For neat text things\nlibrary(gutenbergr)  # For downloading books from Project Gutenberg\n```\n\nWe're going to use the {gutenbergr} package to download some books directly from Project Gutenberg. The IDs for these books come from the URLs at their website. For instance, [*Little Women* is book #514](https://www.gutenberg.org/ebooks/514). We'll store these books as `*_raw` and then clean them up later.\n\n```{r get-text-fake, eval=FALSE}\n# 514 Little Women\nlittle_women_raw <- gutenberg_download(514, meta_fields = \"title\")\n\n# 1524 - Hamlet\n# 1532 - King Lear\n# 1533 - Macbeth\n# 1513 - Romeo and Juliet\ntragedies_raw <- gutenberg_download(c(1524, 1532, 1533, 1513),\n                                    meta_fields = \"title\")\n```\n\n```{r load-saved-text, include=FALSE}\nwithr::with_dir(here::here(), {\n  invisible(list2env(targets::tar_read(gutenberg_books), .GlobalEnv))\n})\n```\n\n::: {.callout-important}\n### Downloading errors\n\nSometimes the Project Gutenberg server gets too much traffic and goes down temporarily (it's all run by volunteers!) and you'll get an error like this:\n\n```default\nWarning: Could not download a book at http://aleph.gutenberg.org/...\n```\n\nProject Gutenberg has multiple copies of itself on different servers around the world, called \"mirrors.\" You can see [the full list of mirrors here](https://www.gutenberg.org/MIRRORS.ALL). If you get an error about connecting to the main Project Gutenberg server (aleph.gutenberg.org), you can specify a different mirror with the `mirror` argument in `gutenberg_download()`:\n\n```{r gutenberg-mirror, eval=FALSE}\nlittle_women_raw <- gutenberg_download(514, meta_fields = \"title\",\n                                       mirror = \"https://gutenberg.pglaf.org/\")\n```\n:::\n\nIf you won't want to redownload the books every time you knit (you don't), you can do the same trick we've used for [WDI](/example/08-example.qmd) and [FRED data](/example/11-example.qmd). Put the actual code for getting the books in a chunk with `eval=FALSE` on it and run it manually in RStudio when you want to get the data. Then you can write the downloaded data as a CSV file, and then load it invisibly from the CSV file when you knit:\n\n````\nI first download data from Project Gutenberg:\n\n```{{r get-book, eval=FALSE}}\nbooks_raw <- gutenberg_download(...)\n\nwrite_csv(books_raw, \"data/books_raw.csv\")\n```\n\n```{{r load-book-data-real, include=FALSE}}\nbooks_raw <- read_csv(\"data/books_raw.csv\")\n```\n\n````\n\n## Clean data\n\nThe data you get from Project Gutenberg comes in a tidy format, with a column for the book id, a column for the title, and a column for text. Sometimes this text column will be divided by lines in the book; sometimes it might be an entire page or paragraph or chapter. It all depends on how the book is formatted at Project Gutenberg.\n\nHere's what the start of our `little_women_raw` data looks like:\n\n```{r show-lw-start}\nhead(little_women_raw)\n```\n\nIf we look at the data in RStudio, we can see that the actual book doesn't start until row 70 (the first 69 rows are the table of contents and other parts of the front matter).\n\n::: {.callout-note}\n### Every book is different!\n\nIn this case, *Little Women* starts at row 67. **That will not be true for all books!** Every book is unique and has different amounts of front matter. You cannot assume that any book you work with starts at line 67.\n:::\n\nIt would be nice if we had a column that indicated what chapter each line is in, since we could then group by chapter and look at patterns within chapters. Since the data doesn't come with a chapter column, we have to make one ourselves using a fun little trick. Each chapter in the book starts with \"CHAPTER ONE\" or \"CHAPTER TWO\", with \"chapter\" in ALL CAPS. We can make a variable named `chapter_start` that will be true if a line starts with \"CHAPTER\" and false if not. Then we can use the `cumsum()` function to take the cumulative sum of this column, which will increment up one number ever time there's a new chapter, thus creating a helpful chapter column.\n\n```{r clean-data-lw}\n# Clean up Little Women\nlittle_women <- little_women_raw %>% \n  # The actual book doesn't start until line 67\n  slice(67:n()) %>% \n  # Get rid of rows where text is missing\n  drop_na(text) %>% \n  # Chapters start with CHAPTER X, so mark if each row is a chapter start\n  # cumsum() calculates the cumulative sum, so it'll increase every time there's\n  # a new chapter and automatically make chapter numbers\n  mutate(chapter_start = str_detect(text, \"^CHAPTER\"),\n         chapter_number = cumsum(chapter_start)) %>% \n  # Get rid of these columns\n  select(-gutenberg_id, -title, -chapter_start)\n\nhead(little_women)\n```\n\nThe data from Shakespeare is similarly messy, with just three columns:\n\n```{r show-shakespeare-raw}\nhead(tragedies_raw)\n```\n\nThe initial text sometimes isn't the actual text of the book. If you look at the beginning of *Hamlet*, for instance, there's a bunch of introductory stuff from editors and transcribers. In real life, we'd want to find a systematic way to get rid of that (perhaps by looking at how many introductory rows there are in each of the four plays and removing those rows), but for now, we'll just live with it and pretend Shakespeare wrote these notes. `r emoji::emoji(\"shrug\")`\n\nWe could also figure out a systematic way to indicate acts and scenes, but that's tricky, so we won't for this example. ([This guy did though!](https://peerchristensen.netlify.app/post/fair-is-foul-and-foul-is-fair-a-tidytext-entiment-analysis-of-shakespeare-s-tragedies/))\n\nNow that we have tidy text data, let's do stuff with it!\n\n\n## Tokens and word counts\n\n### Single words\n\nOne way we can visualize text is to look at word frequencies and find the most common words. This is even more important when looking across documents. \n\nRight now the text we have is tidy, but it is based on lines of text, not words. In order to count words correctly, we need each token (or text element, whether it be a word or bigram or paragraph or whatever) to be in its own row. The `unnest_tokens()` functions from {tidytext} does this for us. The first argument is the name of the column we want to create; the second argument is the name of the column we want to split into tokens.\n\nLet's just work with the Shakespeare tragedies:\n\n```{r tidy-shakespeare-1}\ntragedies_words <- tragedies_raw %>% \n  drop_na(text) %>% \n  unnest_tokens(word, text)\n\nhead(tragedies_words)\n```\n\nNow that we have words, we can filter and count the words. Here's what's happening in this next chunk:\n\n- We use `anti_join()` to remove all common stop words like \"a\" and \"the\" that are listed in the `stop_words` dataset that is loaded when you load {tidytext}\n- We count how many times each word appears in each title/play\n- We only keep the top 15 words\n\n```{r top-15-shakespeare, message=FALSE, warning=FALSE}\ntop_words_tragedies <- tragedies_words %>% \n  # Remove stop words\n  anti_join(stop_words) %>% \n  # Get rid of old timey words and stage directions\n  filter(!(word %in% c(\"thou\", \"thy\", \"haue\", \"thee\", \n                      \"thine\", \"enter\", \"exeunt\", \"exit\"))) %>% \n  # Count all the words in each play\n  count(title, word, sort = TRUE) %>% \n  # Keep top 15 in each play\n  group_by(title) %>% \n  top_n(15) %>% \n  ungroup() %>% \n  # Make the words an ordered factor so they plot in order\n  mutate(word = fct_inorder(word))\ntop_words_tragedies\n```\n\nNow we can plot these results, facetting and filling by title:\n\n```{r plot-top-words-tragedies}\nggplot(top_words_tragedies, aes(y = fct_rev(word), x = n, fill = title)) + \n  geom_col() + \n  guides(fill = \"none\") +\n  labs(y = \"Count\", x = NULL, \n       title = \"15 most frequent words in four Shakespearean tragedies\") +\n  facet_wrap(vars(title), scales = \"free_y\") +\n  theme_bw()\n```\n\nThese results aren't terribly surprising. \"lear\" is the most common word in *King Lear*, \"macbeth\" is the most common word in *Macbeth*, and so on. But the results are still really neat! This is a wordcloud for grownups!\n\n(Sharp-eyed readers will notice that the words aren't actually in perfect order! That's because some common words are repeated across the plays, like \"lord\" and \"sir\". However, each category in a factor can only have one possible position in the orer, so because \"lord\" is the second most common word in *Hamlet* it also appears as #2 in *Macbeth* and *King Lear*. You can fix this with the `reorder_within()` function in {tidytext}—see [Julia Silge's tutorial here](https://juliasilge.com/blog/reorder-within/) for how to use it.)\n\n### Bigrams\n\nWe can also look at pairs of words instead of single words. To do this, we need to change a couple arguments in `unnest_tokens()`, but otherwise everything else stays the same. In order to remove stopwords, we need to split the bigram column into two columns (`word1` and `word2`) with `separate()`, filter each of those columns, and then combine the word columns back together as `bigram` with `unite()`\n\n```{r top-bigrams}\ntragedies_bigrams <- tragedies_raw %>% \n  drop_na(text) %>% \n  # n = 2 here means bigrams. We could also make trigrams (n = 3) or any type of n-gram\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) %>% \n  # Get rid of NAs in the new bigram column\n  drop_na(bigram) %>% \n  # Split the bigrams into two words so we can remove stopwords\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>% \n  filter(!word1 %in% stop_words$word,\n         !word2 %in% stop_words$word) %>% \n  filter(!word1 %in% c(\"thou\", \"thy\", \"thine\", \"enter\", \"exeunt\", \"exit\"),\n         !word2 %in% c(\"thou\", \"thy\", \"thine\", \"enter\", \"exeunt\", \"exit\")) %>% \n  # Put the two word columns back together\n  unite(bigram, word1, word2, sep = \" \")\ntragedies_bigrams\n\ntop_bigrams <- tragedies_bigrams %>% \n  # Count all the bigrams in each play\n  count(title, bigram, sort = TRUE) %>% \n  # Keep top 15 in each play\n  group_by(title) %>% \n  top_n(15) %>% \n  ungroup() %>% \n  # Make the bigrams an ordered factor so they plot in order\n  mutate(bigram = fct_inorder(bigram))\n\nggplot(top_bigrams, aes(y = fct_rev(bigram), x = n, fill = title)) + \n  geom_col() + \n  guides(fill = \"none\") +\n  labs(y = \"Count\", x = NULL, \n       title = \"15 most frequent bigrams in four Shakespearean tragedies\") +\n  facet_wrap(vars(title), scales = \"free\") +\n  theme_bw()\n```\n\nThere are some neat trends here. \"Lord Hamlet\" is the most common pair of words in *Hamlet* (not surprisingly), but in Macbeth the repeated \"knock knock\" (the first non-name repeated pair) is a well-known plot point and reoccurring symbolic theme throughout the play.\n\n## Bigrams and probability\n\nWe can replicate the [\"She Giggles, He Gallops\"](https://pudding.cool/2017/08/screen-direction/) idea by counting the bigrams that match \"he X\" and \"she X\". \n\nThe log ratio idea shows how much more likely a word is compared to its counterpart (so \"he that\" is about 5 more likely to appear than \"she that\". In this graph, I replaced the x-axis labels with \"2x\" and \"4x\", but without those, you get numbers like 1, 2, and 3 (or -1, -2, -3)). To convert those logged ratio numbers into the multiplicative version (i.e. 2x instead of 1), raise 2 to the power of the log ratio. If the log ratio is 3, the human-readable version is $2^3$, or 8 times.\n\n```{r example-logs}\n# Take the log of 8:\nlog2(8)\n\n# Reverse log of 3:\n2^3\n```\n\nThe only text wizardry here is tokenizing the words. Pretty much the rest of all this code is just {dplyr} mutating, filtering, and counting:\n\n```{r bigrams-he-she}\npronouns <- c(\"he\", \"she\")\n\nbigram_he_she_counts <- tragedies_raw %>%\n  drop_na(text) %>% \n  # Split into bigrams\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) %>%\n  # Find counts of bigrams\n  count(bigram, sort = TRUE) %>%\n  # Split the bigram column into two columns\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%\n  # Only choose rows where the first word is he or she\n  filter(word1 %in% pronouns) %>%\n  count(word1, word2, wt = n, sort = TRUE) %>% \n  rename(total = n)\n\nword_ratios <- bigram_he_she_counts %>%\n  # Look at each of the second words\n  group_by(word2) %>%\n  # Only choose rows where the second word appears more than 10 times\n  filter(sum(total) > 10) %>%\n  ungroup() %>%\n  # Spread out the word1 column so that there's a column named \"he\" and one named \"she\"\n  spread(word1, total, fill = 0) %>%\n  # Add 1 to each number so that logs work (just in case any are zero)\n  mutate_if(is.numeric, ~(. + 1) / sum(. + 1)) %>%\n  # Create a new column that is the logged ratio of the she counts to he counts\n  mutate(logratio = log2(she / he)) %>%\n  # Sort by that ratio\n  arrange(desc(logratio))\n\n# Rearrange this data so it's plottable\nplot_word_ratios <- word_ratios %>%\n  # This gets the words in the right order---we take the absolute value, select\n  # only rows where the log ratio is bigger than 0, and then take the top 15 words\n  mutate(abslogratio = abs(logratio)) %>%\n  group_by(logratio < 0) %>%\n  top_n(15, abslogratio) %>%\n  ungroup() %>%\n  mutate(word = reorder(word2, logratio)) \n\n# Finally we plot this\nggplot(plot_word_ratios, aes(y = word, x = logratio, color = logratio < 0)) +\n  geom_segment(aes(y = word, yend = word,\n                   x = 0, xend = logratio), \n               linewidth = 1.1, alpha = 0.6) +\n  geom_point(size = 3.5) +\n  labs(x = \"How much more/less likely\", y = NULL) +\n  scale_color_discrete(name = \"\", labels = c(\"More 'she'\", \"More 'he'\")) +\n  scale_x_continuous(breaks = seq(-3, 3),\n                     labels = c(\"8x\", \"4x\", \"2x\",\n                                \"Same\", \"2x\", \"4x\", \"8x\")) +\n  theme_bw() +\n  theme(legend.position = \"bottom\")\n```\n\nShakespeare doesn't use a lot of fancy verbs in his plays, so we're left with incredibly common verbs like \"should\" and \"comes\" and \"was\". Oh well.\n\n\n## Term frequency-inverse document frequency (tf-idf)\n\nWe can determine which words are the most unique for each book/document in our corpus using by calculating the tf-idf (term frequency-inverse document frequency) score for each term. The tf-idf is the product of the term frequency and the inverse document frequency:\n\n$$\n\\begin{aligned}\n\\operatorname{tf}(\\text{term}) &= \\frac{n_{\\text{term}}}{n_{\\text{terms in document}}} \\\\\n\\operatorname{idf}(\\text{term}) &= \\ln{\\left(\\frac{n_{\\text{documents}}}{n_{\\text{documents containing term}}}\\right)} \\\\\n\\operatorname{tf-idf}(\\text{term}) &= \\operatorname{tf}(\\text{term}) \\times \\operatorname{idf}(\\text{term})\n\\end{aligned}\n$$\n\nFortunately you don't need to remember that formula. The `bind_tf_idf()` function will calculate this for you. Remember, the higher the tf-idf number, the more unique the term is in the document, but these numbers are meaningless and unitless—you can't convert them to a percentage or anything.\n\nHere are the most unique words in these four tragedies, compared to all the tragedies:\n\n```{r tf-idf-tragedies, message=FALSE}\ntragedy_words <- tragedies_raw %>% \n  drop_na() %>% \n  # Split into word tokens\n  unnest_tokens(word, text) %>% \n  # Remove stop words and old timey words\n  anti_join(stop_words) %>% \n  filter(!word %in% c(\"thou\", \"thy\", \"haue\", \"thee\", \n                      \"thine\", \"enter\", \"exeunt\", \"exit\")) %>% \n  count(title, word, sort = TRUE)\n\n# Add the tf-idf values to the counts\ntragedy_tf_idf <- tragedy_words %>% \n  bind_tf_idf(word, title, n)\n\n# Get the top 10 uniquest words\ntragedy_tf_idf_plot <- tragedy_tf_idf %>% \n  arrange(desc(tf_idf)) %>% \n  group_by(title) %>% \n  top_n(10) %>% \n  ungroup() %>% \n  mutate(word = fct_inorder(word))\n\nggplot(tragedy_tf_idf_plot, \n       aes(y = fct_rev(word), x = tf_idf, fill = title)) +\n  geom_col() +\n  guides(fill = \"none\") +\n  labs(x = \"tf-idf\", y = NULL) +\n  facet_wrap(~ title, scales = \"free\") +\n  theme_bw()\n```\n\nNot surprisingly, the most unique words for each play happen to be the names of the characters in those plays.\n\n\n## Sentiment analysis\n\nIn the video, I plotted the sentiment of *Little Women* across the book, but it wasn't a very interesting plot. We'll try with Shakespeare here instead. \n\nAt its core, sentiment analysis involves looking at a big list of words for how negative or positive they are. Some sentiment dictionaries mark if a word is \"negative\" or \"positive\"; some give words a score from -3 to 3; some give different emotions like \"sadness\" or \"anger\". You can see what the different dictionaries look like with `get_sentiments()`\n\n```{r show-dictionary}\nget_sentiments(\"afinn\")  # Scoring system\n# get_sentiments(\"bing\")  # Negative/positive\n# get_sentiments(\"nrc\")  # Specific emotions\n# get_sentiments(\"loughran\")  # Designed for financial statements; positive/negative\n```\n\nHere we split the Shakespearean tragedies into words, join a sentiment dictionary to it, and use {dplyr} data wrangling to calculate the net number positive words in each chapter. Had we used the AFINN library, we could calculate the average sentiment per chapter, since AFINN uses a scoring system instead of negative/positive labels. Or we could've used the NRC library, which has specific emotions like trust and fear.\n\n```{r shakespeare-sentiment, message=FALSE}\ntragedy_words <- tragedies_raw %>% \n  drop_na() %>% \n  # Split into word tokens\n  unnest_tokens(word, text) %>% \n  # Remove stop words and old timey words\n  anti_join(stop_words) %>% \n  filter(!word %in% c(\"thou\", \"thy\", \"haue\", \"thee\", \n                      \"thine\", \"enter\", \"exeunt\", \"exit\"))\n\n# Join the sentiment dictionary \ntragedy_sentiment <- tragedy_words %>% \n  inner_join(get_sentiments(\"bing\"), relationship = \"many-to-many\")\ntragedy_sentiment\n```\n\nWe can look at these sentiments a few different ways. First we can get a count of total positive and negative words in the four books. We can see that in all four, there are more negative words than positive ones (they're tragdies, after all):\n\n```{r shakespeare-sentiment-plot-bars, message=FALSE, warning=FALSE}\ntragedy_sentiment_plot <- tragedy_sentiment %>% \n  count(title, sentiment)\n\nggplot(tragedy_sentiment_plot, aes(x = sentiment, y = n, fill = title, alpha = sentiment)) +\n  geom_col(position = position_dodge()) +\n  scale_alpha_manual(values = c(0.5, 1)) +\n  facet_wrap(vars(title)) +\n  theme_bw()\n```\n\nPerhaps more usefully, we can divide each of the plays into groups of 100 lines, and then get the net sentiment of each group (number of positive words − number of negative words). By splitting the data into groups of lines, we can show a more granular view of the progression of the plot. To do this we make a column that indicates the row number, and then we use the special `%/%` operator to perform integer division, which essentially lops off the decimal point when dividing numbers: 150/100 normally is 1.5, but in integer divison, it is 1. This is a helpful trick for putting rows 1-99 in one group, then rows 100-199 in another group, etc.\n\n```{r shakespeare-sentiment-plot-lines, message=FALSE}\ntragedies_split_into_lines <- tragedy_sentiment %>% \n  # Divide lines into groups of 100\n  mutate(line = row_number(),\n         line_chunk = line %/% 100) %>% \n  # Get a count of postiive and negative words in each 100-line chunk in each play\n  count(title, line_chunk, sentiment) %>% \n  # Convert the sentiment column into two columns named \"positive\" and \"negative\"\n  pivot_wider(names_from = sentiment, values_from = n) %>% \n  # Calculate net sentiment\n  mutate(sentiment = positive - negative)\n\nggplot(tragedies_split_into_lines,\n       aes(x = line_chunk, y = sentiment, fill = sentiment)) +\n  geom_col() +\n  scale_fill_viridis_c(option = \"magma\", end = 0.9) +\n  facet_wrap(vars(title), scales = \"free_x\") +\n  theme_bw()\n```\n\nNeat. They're all really sad and negative, except for the beginning of Romeo and Juliet where the two lovers meet and fall in love. Then everyone dies later.\n\n\n## Neat extra stuff\n\nNone of this stuff was in the video, but it's useful to know and see how to do it. It all generally comes from the [*Tidy Text Mining* book](https://www.tidytextmining.com/) by Julia Silge and David Robinson\n\n### Part of speech tagging\n\nR has no way of knowing if words are nouns, verbs, or adjectives. You can algorithmically predict what part of speech each word is using a part-of-speech tagger, like [spaCy](https://spacy.io/) or [Stanford's Natural Langauge Processing (NLP) library](https://nlp.stanford.edu/). \n\nThese are external programs that are not written in R and don't naturally communicate with R (spaCy is written in Python; Stanford's CoreNLP is written in Java). There is a helpful R package named {cleanNLP} that helps you interact with these programs from within R, whis is super helpful. {cleanNLP} also comes with its own R-only tagger so you don't need to install anything with Python or Java (however, it's not as powerful as either spaCy, which is faster, and doesn't deal with foreign languages like Arabic and Chinese like Stanford's NLP library).\n\nYou can see other examples of part-of-speech tagging (along with instructions for how to install spaCy and coreNLP) here:\n\n- {{< fa arrow-up-right-from-square >}} [\"Tidy text, parts of speech, and unique words in the Bible\"](https://www.andrewheiss.com/blog/2018/12/26/tidytext-pos-john/)\n- {{< fa arrow-up-right-from-square >}} [\"Tidy text, parts of speech, and unique words in the Qur'an\"](https://www.andrewheiss.com/blog/2018/12/28/tidytext-pos-arabic/)\n\nHere's the general process for tagging (or \"annotating\") text with the {cleanNLP} package:\n\n1. Make a dataset where one column is the id (line number, chapter number, book+chapter, etc.), and another column is the text itself.\n\n2. Initialize the NLP tagger. You can use any of these:\n\n    - `cnlp_init_udpipe()`: Use an R-only tagger that should work without installing anything extra (a little slower than the others, but requires no extra steps!)\n    - `cnlp_init_spacy()`: Use spaCy (if you've installed it on your computer with Python)\n    - `cnlp_init_corenlp()`: Use Stanford's NLP library (if you've installed it on your computer with Java) \n\n3. Feed the data frame from step 1 into the `cnlp_annotate()` function and wait.\n\n4. Save the tagged data on your computer so you don't have to re-tag it every time.\n\nHere's an example using the *Little Women* data:\n\n```{r lw-reshape}\n# For the tagger to work, each row needs to be unique, which means we need to\n# combine all the text into individual chapter-based rows. This takes a little\n# bit of text-wrangling with dplyr:\nlittle_women_to_tag <- little_women %>% \n  # Group by chapter number\n  group_by(chapter_number) %>% \n  # Take all the rows in each chapter and collapse them into a single cell\n  nest(data = c(text)) %>% \n  ungroup() %>% \n  # Look at each individual cell full of text lines and paste them together into\n  # one really long string of text per chapter\n  mutate(text = map_chr(data, ~paste(.$text, collapse = \" \"))) %>% \n  # Get rid of this column\n  select(-data)\nlittle_women_to_tag\n```\n\nNotice how there's now a row for each chapter, and the whole chapter is contained in the `text` column. With the data in this format, we can annotate it. It takes 75 seconds to run this on my 2021 MacBook Pro with the R-only udpipe tagger (and only 30 seconds if I use the spaCy tagger). Notice how I immediately save the tagged tokens as a CSV file after so I don't have to do it again. \n\n```{r nlp-example-tag, eval=FALSE}\nlibrary(cleanNLP)\n\n# Use the built-in R-based tagger\ncnlp_init_udpipe()\n\nlittle_women_tagged_raw <- cnlp_annotate(little_women_to_tag, \n  text_name = \"text\", \n  doc_name = \"chapter_number\")\n\n# Save the tagged token dataframe so we don't have to run this again\nwrite_csv(little_women_tagged_raw$token, \"data/little_women_tagged.csv\")\n\n# Load the tagged tokens\nlittle_women_tagged <- read_csv(\"data/little_women_tagged.csv\")\n```\n\nHere's what the tagged text looks like:\n\n```{r show-tagged-lw}\nlittle_women_tagged\n```\n\nThere are a bunch of new columns like `lemma` (or the base stemmed word), and `upos` and `pos` for the different parts of speech. These use the [Penn Treebank codes](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).\n\nNow that everything is tagged, we can do any grouping and summarizing and filtering we want. We could find the most common verbs, or the most common nouns or proper names, for instance. Here's a fun plot that shows the proportion of mentions of the four main characters (Meg, Jo, Beth, and Amy) in each chapter.\n\n```{r lw-props}\n# Find all proper nouns\nproper_nouns <- little_women_tagged %>% \n  filter(upos == \"PROPN\")\n\nmain_characters_by_chapter <- proper_nouns %>% \n  # Find only Meg, Jo, Beth, and Amy\n  filter(lemma %in% c(\"Meg\", \"Jo\", \"Beth\", \"Amy\")) %>% \n  # Group by chapter and character name\n  group_by(doc_id, lemma) %>% \n  # Get the count of mentions\n  summarize(n = n()) %>% \n  # Make a new column named \"name\" that is an ordered factor of the girls' names\n  mutate(name = factor(lemma, levels = c(\"Meg\", \"Jo\", \"Beth\", \"Amy\"), ordered = TRUE)) %>% \n  # Rename this so it's called chapter\n  rename(chapter = doc_id) %>% \n  # Group by chapter\n  group_by(chapter) %>% \n  # Calculate the proportion of each girl's mentions in each chapter\n  mutate(prop = n / sum(n)) %>% \n  ungroup() %>% \n  # Make a cleaner chapter name column\n  mutate(chapter_name = paste(\"Chapter\", chapter)) %>% \n  mutate(chapter_name = fct_inorder(chapter_name))\nmain_characters_by_chapter\n```\n\nAnd here's the polished plot:\n\n```{r lw-props-plot, fig.width=8, fig.height=5}\nggplot(main_characters_by_chapter, aes(x = prop, y = \"1\", fill = fct_rev(name))) + \n  geom_col(position = position_stack()) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_discrete(expand = c(0, 0)) +\n  scale_fill_viridis_d(option = \"plasma\", end = 0.9, name = NULL) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(x = NULL, y = NULL,\n       title = \"Proportion of mentions of each\\nLittle Woman per chapter\",\n       subtitle = \"Jo basically dominates the last third of the book\") +\n  facet_wrap(vars(chapter_name), nrow = 6) +\n  theme_bw(base_family = \"Roboto Condensed\") +\n  theme(legend.position = \"top\",\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        strip.background = element_rect(fill = \"white\"),\n        legend.text = element_text(face = \"bold\", size = rel(1)),\n        plot.title = element_text(face = \"bold\", hjust = 0.5, size = rel(1.7)),\n        plot.subtitle = element_text(hjust = 0.5, size = rel(1.1)))\n```\n\n\n### Topic modeling and fingerprinting\n\nIf you want to see some examples of topic modeling with Latent Dirichlet Allocation (LDA) or text fingerprinting based on sentence length and counts of hapax legomena ([based on this article](https://kops.uni-konstanz.de/bitstream/handle/123456789/5492/Literature_Fingerprinting.pdf)), see these examples from a previous version of this class: [topic modeling](https://datavizf18.classes.andrewheiss.com/class/11-class/#topic-modeling) and [fingerprinting](https://datavizf18.classes.andrewheiss.com/class/11-class/#fingerprinting).\n\n\n### Text features\n\nFinally, you can use [the {textfeatures} package](https://github.com/mkearney/textfeatures) to find all sorts of interesting numeric statistics about text, like the number of exclamation points, commas, digits, characters per word, uppercase letters, lowercase letters, and more!\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"highlight-style":"monokai","toc-depth":4,"filters":["../filters/format_date_end.lua"],"output-file":"13-example.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","bibliography":["../files/bib/readings.bib"],"csl":"../files/bib/chicago-author-date.csl","_quarto-vars":{"author":"Andrew Heiss","instructor":{"name":"Dr. Andrew Heiss","name_no_title":"Andrew Heiss","email":"aheiss@gsu.edu","url":"https://www.andrewheiss.com","twitter":"andrewheiss","github":"andrewheiss","office":"55 Park Place SE, Room 464","contact_policy":"E-mail and Slack are the best ways to get in contact with me. I will try to respond to all course-related e-mails and Slack messages within 24 hours (*really*), but also remember that life can be busy and chaotic for everyone (including me!), so if I don't respond right away, don't worry!","appointment_url":"https://calendly.com/andrewheiss/"},"course":{"number":"PMAP 8551/4551","semester":"Fall 2023","days":"Any day","time":"Asynchronous","location":"Anywhere","dates":"August 12–December 11, 2023","github":"https://www.github.com/andrewheiss/datavizf23.classes.andrewheiss.com","url":"https://datavizf23.classes.andrewheiss.com","copyright_year":"2023","slack":"https://gsudatavizf2023.slack.com"},"university":{"name":"Georgia State University","url":"https://www.gsu.edu"},"school":{"name":"Andrew Young School of Policy Studies","url":"https://aysps.gsu.edu/"}},"theme":["litera","../html/custom.scss"],"date-heading":{"content":"Example for","class":"bg-example"},"date-format":"full","template-partials":["../html/title-block.html"],"title":"Text","date":"2023-11-13","date_end":"2023-11-17"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}