{"title":"Weeks 11, 12, and 13 tips and FAQs","markdown":{"yaml":{"title":"Weeks 11, 12, and 13 tips and FAQs","date":"2023-11-29T14:34","categories":["FAQs"],"toc-depth":4},"headingText":"Can I use `geom_label_repel()` with maps?","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  fig.width = 6, \n  fig.height = 6 * 0.618, \n  fig.align = \"center\", \n  out.width = \"90%\",\n  collapse = TRUE\n)\n```\n\nHi everyone!\n\nJust a few quick tips from the past few weeks:\n\n\n\nYou learned about the {ggrepel} package in [session 9](/example/09-example.qmd), with its `geom_text_repel()` and `geom_label_repel()` functions that make sure none of your labels overlap:\n\n```{r libraries-data, warning=FALSE, message=FALSE}\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(ggrepel)\n\nsmall_mpg <- mpg %>% \n  # Only use the first 10 rows\n  slice(1:10) %>% \n  # Make a label column\n  mutate(fancy_label = paste0(manufacturer, \" \", model, \" (\", year, \")\"))\n\nggplot(small_mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  geom_label_repel(aes(label = fancy_label), seed = 1234)\n```\n\nIn [session 12](/example/12-example.html), you learned about `geom_sf_text()` and `geom_sf_label()` for adding text and labels to maps. But what if your map labels overlap, like this?\n\n```{r counties-fake, eval=FALSE}\n# Download cb_2022_us_county_5m.zip under \"County\" from\n# https://www.census.gov/geographies/mapping-files/time-series/geo/cartographic-boundary.html\nga_counties <- read_sf(\"data/cb_2022_us_county_5m/cb_2022_us_county_5m.shp\") %>% \n  filter(STATEFP == 13)\n```\n\n```{r counties-real, include=FALSE}\nga_counties <- read_sf(here::here(\"files\", \"data\", \"external_data\", \"maps\",\n                                  \"cb_2022_us_county_5m\",\n                                  \"cb_2022_us_county_5m.shp\")) %>% \n  filter(STATEFP == 13)\n```\n\n```{r ga-places-label-overlap, warning=FALSE}\nga_places <- tribble(\n  ~city, ~lat, ~long,\n  \"Atlanta\", 33.748955, -84.388099,\n  \"Alpharetta\", 34.075318, -84.294105,\n  \"Duluth\", 34.002262, -84.143614\n) %>% \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(\"EPSG:4326\"))\n\nggplot() +\n  geom_sf(data = ga_counties, linewidth = 0.1) +\n  geom_sf(data = ga_places) +\n  geom_sf_label(data = ga_places, aes(label = city)) +\n  theme_void()\n```\n\nUnfortunately there's no such thing as `geom_sf_label_repel()`. BUT there's still a way to use `geom_label_repel()` and `geom_text_repel()` with maps, with a couple little tweaks:\n\n1. You have to map the `geometry` column in the data to the `geometry` aesthetic in `geom_text/label_repel()`\n2. You have to tell `geom_text/label_repel()` to use the \"sf_coordinates\" stat so that it uses the latitude and longitude coordinates for x/y\n\n```{r ga-places-label-fixed, warning=FALSE}\nggplot() +\n  geom_sf(data = ga_counties, linewidth = 0.1) +\n  geom_sf(data = ga_places) +\n  geom_label_repel(\n    data = ga_places,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\", seed = 1234\n  ) +\n  theme_void()\n```\n\n\n### I tried to make a map and countries are missing—why?\n\nMany of you were brave and made a map of refugee counts for mini project 2. That's fantastic!\n\nIf you did, you likely ran into an issue with plotting the countries and getting an incomplete map. Here's an example with our beloved gapminder data.\n\n```{r load-gapminder-map-data-fake, eval=FALSE}\nlibrary(countrycode)  # For dealing with country names, abbreviations, and codes\nlibrary(gapminder)    # Global health and wealth\n\n# Add an ISO country code column to gapminder for joining\ngapminder_clean <- gapminder %>% \n  mutate(ISO_A3 = countrycode(country, \"country.name\", \"iso3c\"))\n\n# Load the world map data from exercise 12\n# Download \"Admin 0 – Countries\" from\n# https://www.naturalearthdata.com/downloads/110m-cultural-vectors/\nworld_map <- read_sf(\"data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp\") %>% \n  filter(ISO_A3 != \"ATA\") %>%  # Bye penguins\n  mutate(ISO3 = ADM0_A3)  # Use ADM0_A3 as the main country code column\n```\n\n```{r load-gapminder-map-real, include=FALSE}\nlibrary(countrycode)\nlibrary(gapminder)\n\ngapminder_clean <- gapminder %>% \n  mutate(ISO3 = countrycode(country, \"country.name\", \"iso3c\"))\n\nworld_map <- read_sf(here::here(\"files\", \"data\", \"external_data\", \"maps\",\n                                \"ne_110m_admin_0_countries\", \n                                \"ne_110m_admin_0_countries.shp\")) %>% \n  filter(ISO_A3 != \"ATA\") %>% \n  mutate(ISO3 = ADM0_A3)\n```\n\nLet's take just 2007 from gapminder and map life expectancy. To do this we'll need to combine or join the two datasets. One logical way to do this would be to take gapminder, join the world map data to it, and then plot it:\n\n```{r gapminder-2007-error, error=TRUE}\ngapminder_with_map <- gapminder_clean %>% \n  filter(year == 2007) %>% \n  left_join(world_map, by = join_by(ISO3))\n\nggplot() +\n  geom_sf(data = gapminder_with_map, aes(fill = lifeExp))\n```\n\noh no there's an error! When we joined the map data, the special attributes of the `geometry` column in `world_map` got lost. The column is still there, but it won't automatically plot with `geom_sf()`. We can fix that by specifying that the column named \"geometry\" does indeed contain all the geographic data with `st_set_geometry()`:\n\n```{r gapminder-2007-missing}\ngapminder_with_map <- gapminder_clean %>% \n  filter(year == 2007) %>% \n  left_join(world_map, by = join_by(ISO3)) %>% \n  # Fix the geometry column\n  st_set_geometry(\"geometry\")\n\nggplot() +\n  geom_sf(data = gapminder_with_map, aes(fill = lifeExp)) +\n  theme_void()\n```\n\nWe have a… map? It's missing a bunch of countries (Russia is the most glaringly obvious hole!). That's because those countries aren't in gapminder, so their corresponding maps didn't come over when using `left_join()`. We can confirm by counting rows. The original map data has maps for 176 countries. Gapminder has 142 countries in 2007. The combined `gapminder_with_map` dataset only has 142 rows—we're not plotting 34 countries, since they're not in gapminder.\n\n```{r count-rows}\nnrow(world_map)\nnrow(gapminder_clean %>% filter(year == 2007))\nnrow(gapminder_with_map)\n```\n\nOne quick and easy way to fix this is to use two `geom_sf()` layers: one with the whole world and one with the partial gapminder-only map:\n\n```{r two-geom-sf-layers}\nggplot() +\n  geom_sf(data = world_map) +\n  geom_sf(data = gapminder_with_map, aes(fill = lifeExp)) +\n  theme_void()\n```\n\nThe *better* way to fix this is to join the two datasets in a different order—start with the full map data and then add gapminder to it. This maintains the specialness of the geometry column and keeps all the original rows in `world_map`. For countries that are in the map data but not in gapminder, they'll still be in the final `map_with_gapminder` data, but they'll have NA for life expectancy:\n\n```{r gapminder-2007-good}\nmap_with_gapminder <- world_map %>% \n  left_join(filter(gapminder_clean, year == 2007), by = join_by(ISO3))\n\nggplot() +\n  geom_sf(data = map_with_gapminder, aes(fill = lifeExp)) +\n  theme_void() +\n  # Make the countries with missing data a different color\n  scale_fill_gradient(na.value = \"grey90\")\n```\n\nWhat if we want to facet though? This is just one year—what if we want to show panels for multiple years? This gets a little tricky. The gapminder data has rows for different country/year combinations (Afghanistan 1952, Afghanistan 1957, Albania 1952, etc.), but the world map data only has rows for countries. If we join the gapminder data to the world map data and gapminder has multiple rows for years, there's no clear place for the gapminder rows to connect with the world map rows. R will try to make it work and repeat world_map rows for each of the repeated years, but it can be unpredictable.\n\nThe best approach I've found for doing this is to create what I call a \"skeleton\" data frame that has all the possible combinations of (1) unique countries in the map data and (2) unique years in gapminder (or the refugee data if you're using that). The `expand_grid()` function does this automatically. Like, look what happens if we tell it to make rows for every combination of A, B, C and 1, 2, 3—we get A1, A2, A3, B1, B2, and so on:\n\n```{r expand-grid-example}\nexpand_grid(column_1 = c(\"A\", \"B\", \"C\"),\n            column_2 = c(1, 2, 3))\n```\n\nWe'll make a similar skeleton with all the countries in the map and all the years we care about in gapminder. We'll just show two panels—1952 and 2007—so we'll make a little filtered dataset first. Then we'll use `expand_grid()` to make a dataset with all those combinations: Afghanistan 1952, Afghanistan 2007, Albania 1952, Albania 2007, and so on:\n\n```{r create-initial-skeleton}\ngapminder_smaller <- gapminder_clean %>%\n  filter(year %in% c(1952, 2007))\n\nskeleton <- expand_grid(ISO3 = unique(world_map$ISO3),\n                        year = unique(gapminder_smaller$year))\nskeleton\n```\n\nNeat, that works. There's Fiji in 1952 and 2007, Tanzania in 1952 and 2007, and so on. Those are all the possible countries in `world_map` with all the possible years in `gapminder_smaller`.\n\nNext we can join in the gapminder data for each country and year, and join in the map data for each country. Notice how it has the same number of rows as `skeleton` (352). If a country doesn't have gapminder data (like Fiji here), it gets an NA for `lifeExp` and `pop` and `gdpPercap`. But it still has map data for both 1952 and 2007, so it'll show up in a plot.\n\n```{r make-full-gapminder-data}\nfull_gapminder_map <- skeleton %>% \n  left_join(gapminder_smaller, by = join_by(ISO3, year)) %>%\n  left_join(world_map, by = join_by(ISO3)) %>% \n  # The geometry column lost its magic powers after joining, so add it back\n  st_set_geometry(\"geometry\")\nfull_gapminder_map\n```\n\nNow we can plot it and we'll have consistent countries in each panel:\n\n```{r gapminder-map-fixed}\nggplot() +\n  geom_sf(data = full_gapminder_map, aes(fill = lifeExp)) +\n  facet_wrap(vars(year), ncol = 1) +\n  scale_fill_gradient(na.value = \"grey90\") +\n  theme_void()\n```\n\nPerfect!\n\n\n### Some of the words in my word frequency/tf-idf plot were out of order—how can I fix that?\n\nIn the [example for week 13](/example/13-example.html#tokens-and-word-counts), I showed the 15 most frequent words in Hamlet, Macbeth, Romeo and Juliet, and King Lear, faceted by play. Only Romeo and Juliet, though, has the words in the correct order. The other plays have strange ordering. Note how \"lord\" and \"king\" are weirdly misplaced in Macbeth and Hamlet and how \"love\" is weirdly misplaced in Hamlet:\n\n![Words in the wrong order across panels](img/plot-top-words-tragedies-annotated.png)\n\nThe word \"lord\" is the second most common word in Hamlet, so R thinks it is the second most common word across all the plays. It doesn't know that there's a difference between \"lord\" in Hamlet and \"lord\" in Macbeth. As a result, any common words that are shared across the plays will appear out of order.\n\nThis is fixable though! [See this blog post by Julia Silge](https://juliasilge.com/blog/reorder-within/), one of the authors of {tidytext}. Basically, you need to use `reorder_within()` to sort the words correctly inside each play, then add `scale_y_reordered()` to make them display correctly.\n\nHere's what that looks like with the Shakespeare words.\n\n```{r libraries-data-reordering, warning=FALSE, message=FALSE}\nlibrary(tidyverse)\nlibrary(gutenbergr)  # For getting books from Project Gutenberg\nlibrary(tidytext)    # For working with text\n```\n\n```{r get-text-fake, eval=FALSE}\ntragedies_raw <- gutenberg_download(\n  c(\n    1524,  # Hamlet\n    1532,  # King Lear\n    1533,  # Macbeth\n    1513   # Romeo and Juliet\n  ),\n  meta_fields = \"title\"\n)\n```\n\n```{r load-saved-text, include=FALSE}\nwithr::with_dir(here::here(), {\n  invisible(list2env(targets::tar_read(gutenberg_books), .GlobalEnv))\n})\n```\n\n```{r clean-tragedies, message=FALSE}\n# Clean up the tragedies text\ntop_words_tragedies <- tragedies_raw %>% \n  drop_na(text) %>% \n  unnest_tokens(word, text) %>% \n  # Remove stop words\n  anti_join(stop_words) %>% \n  # Get rid of old timey words and stage directions\n  filter(!(word %in% c(\"thou\", \"thy\", \"haue\", \"thee\", \n                      \"thine\", \"enter\", \"exeunt\", \"exit\"))) %>% \n  # Count all the words in each play\n  count(title, word, sort = TRUE) %>% \n  # Keep top 15 in each play\n  group_by(title) %>% \n  top_n(15) %>% \n  ungroup()\ntop_words_tragedies\n```\n\nBecause we used `top_n()`, these words are already sorted in order of frequency (with \"hamlet\" appearing the most at 461 times). In example 13, we locked in that order by making the `word` column an ordered factor, like this:\n\n```{r plot-top-words-tragedies-wrong}\ntop_words_tragedies_order_wrong <- top_words_tragedies %>%\n  # Make the words an ordered factor so they plot in order\n  mutate(word = fct_inorder(word)) \n\nggplot(top_words_tragedies_order_wrong, aes(y = fct_rev(word), x = n, fill = title)) + \n  geom_col() + \n  guides(fill = \"none\") +\n  labs(y = \"Count\", x = NULL, \n       title = \"15 most frequent words in four Shakespearean tragedies\") +\n  facet_wrap(vars(title), scales = \"free_y\") +\n  theme_bw()\n```\n\nBut that's wrong!\n\nInstead of using `fct_inorder()`, we need to use `reorder_within()` and tell it to sort the words by count within each play:\n\n```{r tragedies-reorder-within}\ntop_words_tragedies_order_right <- top_words_tragedies %>%\n  # Make the words an ordered factor so they plot in order\n  mutate(word = reorder_within(word, n, title)) \n\ntop_words_tragedies_order_right\n```\n\nNotice how the `word` column looks a little weird now. It added the play name to the end of each word, like `macbeth___Macbeth`. That's actually a creative hack for fixing the ordering. Remember that the main reason the ordering is messed up across facets is because R doesn't know that the word \"love\" in Hamlet is different from the word \"love\" in Romeo and Juliet. By changing the words to `love___Romeo and Juliet` and `love___Hamlet`, R can now recognize the different versions of \"love\" and sort them correctly. Let's plot this version:\n\n```{r plot-top-words-tragedies-right-but-wrong}\nggplot(top_words_tragedies_order_right, aes(y = word, x = n, fill = title)) + \n  geom_col() + \n  guides(fill = \"none\") +\n  labs(y = \"Count\", x = NULL, \n       title = \"15 most frequent words in four Shakespearean tragedies\") +\n  facet_wrap(vars(title), scales = \"free_y\") +\n  theme_bw()\n```\n\noh no.\n\nThe order is right (yay!) but the y-axis is horrible since it's including the hacky `___play name` at the end of each of the words. \n\nTo fix that, we can use `scale_y_reordered()`, which cleans up those word labels by removing the three underscores and any text that follows them:\n\n```{r plot-top-words-tragedies-right}\nggplot(top_words_tragedies_order_right, aes(y = word, x = n, fill = title)) + \n  geom_col() + \n  scale_y_reordered() +\n  guides(fill = \"none\") +\n  labs(y = \"Count\", x = NULL, \n       title = \"15 most frequent words in four Shakespearean tragedies\") +\n  facet_wrap(vars(title), scales = \"free_y\") +\n  theme_bw()\n```\n\nPerfect!\n\n\n### Cleaning up text is *always* specific and specialized\n\nIn the Shakespeare example, we removed common stop words like \"the\" and \"a\" with `anti_join()` and then manually removed some other more specific words like \"thou\" and \"thee\" and \"exit\":\n\n```{r old-timey-words, eval=FALSE}\n# Clean up the tragedies text\ntop_words_tragedies <- tragedies_raw %>% \n  drop_na(text) %>% \n  unnest_tokens(word, text) %>% \n  # Remove stop words\n  anti_join(stop_words) %>% \n  # Get rid of old timey words and stage directions\n  filter(!(word %in% c(\"thou\", \"thy\", \"haue\", \"thee\", \n                       \"thine\", \"enter\", \"exeunt\", \"exit\")))\n```\n\nThat's because in these specific plays, those are common words that we want to ignore—they're basically our own custom stop words. We should also probably get rid of words like \"act\" and \"scene\" too, but we didn't here.\n\nMany of you kept that exact code in exercise 13, removing \"thou\", \"thy\", \"exeunt\", and those other words from your own text. But that's not necessary or helpful. If you're working with something like Harry Potter or Jane Austen or Ernest Hemmingway or anything more modern than Shakespeare, those words aren't really in there. In the Shakespeare example, we removed \"enter\" and \"exit\" because those are stage directions, but in other books those are regular actual words and probably shouldn't be removed.\n\nThere's no one universal set of stop words that you can use—every text is unique and has its own quirks that you need to take care of. \n\nFor example, one of you looked at four books by W. E. B. Du Bois and did this to clean up the stop words:\n\n```{r dubois-words, eval=FALSE}\ndubois_clean %>%\n  anti_join(stop_words) %>% \n  filter(!(word %in% c(\"1\", \"2\", \"cong\", \"sess\", \"act\", \"pp\", \"_ibid\",\n                       \"_house\", \"3\", \"doc\")))\n```\n\nThat's awesome. Those are all words that are specific to those four books and that were likely appearing in the frequency plot. One (or more) of the books probably mentioned lots of congressional activity, like congressional sessions, acts of congress, stuff happening in the House of Representatives, and so on. There were probably also a lot of citations, with things like \"pp.\" (the abbreviation for \"pages\", like \"pp. 125-127\") and \"ibid\" (the abbreviation for \"see the previous citation\"). That list of words is specific to those four books and *should not* be applied to other books—like, there's no reason to remove those words from the Shakespeare tragedies or from Little Women or from Harry Potter because none of those mention congressional sessions or use \"ibid\".\n\nData cleaning is *always* context specific.\n\n\n### Saving data that takes a long time to make\n\nIn these later sessions, I've had you do things with data from different places on the internet. In [exercise 13](/example/13-example.qmd) you grabbed books from Project Gutenberg. Some of you used [`osrmRoute()`](https://www.andrewheiss.com/blog/2023/06/01/geocoding-routing-openstreetmap-r/#routing) in exercise 12 to create a mapped route between cities. Some of you used [{tidygeocoder}](/example/12-example.qmd#automatic-geocoding-by-address) to geocode addresses in exercise 12. In past sessions you've used `WDI()` to download data from the World Bank.\n\nWhen you knit a document, R starts with a brand new empty session without any packages or data loaded, and then it runs all your chunks to load packages, load data, and run all your other code. If you have code that grabs data from the internet, **it will run every time you knit your document**. [Remember my suggestion to knit often](/news/2023-10-03_cleaner-nicer-rmd-output.qmd#knit-often)? You'll re-download the data, re-create routes, re-geocode addresses, and so on every time you keep re-knitting. This is excessive, slow, and—most especially—bad R etiquette. You don't want to keep accessing those servers and recalculate things and redownload things you don't need to update. \n\nBUT at the same time, you should care about reproducibility. You want others—and future you—to be able to run your code and create the same plots and tables and get the same data. But you don't want to do all that excessively and impolitely.\n\nThe solution is to be a little tricky with your R Markdown file. If you have code that needs to grab something from the internet, put it in a chunk that doesn't run—use `eval=FALSE` in its chunk options. Then, in an invisible chunk (with `include=FALSE`) load the pre-downloaded data manually. I showed this in [example 8](/example/08-example.qmd#load-and-clean-data) (with {WDI}) and [example 11](/example/11-example.qmd#get-data) (with {tidyquant}) and [example 13](/example/13-example.qmd#get-data) (with {gutenberger})\n\nHere's a quick basic example with Project Gutenberg book data. There are two chunks: `get-book-fake` and `load-book-real`: \n\n````markdown\n```{{r get-book-fake, eval=FALSE}}\nlittle_women_raw <- gutenberg_download(514, meta_fields = \"title\")\n```\n\n```{{r load-book-data-real, include=FALSE}}\nlittle_women_file <- \"data/little_women_raw.csv\"\n\nif (file.exists(little_women_file)) {\n  little_women_raw <- read_csv(little_women_file)\n} else {\n  little_women_raw <- gutenberg_download(514, meta_fields = \"title\")\n  \n  write_csv(little_women_raw, little_women_file)\n}\n```\n````\n\n1. The first chunk (`get-book-fake`) contains the code for downloading data with `gutenberg_download()`. It will appear in the document, but **it will not run** since it has `eval=FALSE` on. It will not try to grab anything from the internet. If someone were to follow along with the code in your document, they could run that code and get the book (good!), and it won't run repeatedly on your end (also good!).\n2. The second chunk (`load-book-data-real`) does a neat little trick. It first checks to see if a CSV file named `data/little_women_raw.csv` exists. If it does, it'll just load it with `read_csv()`. If it doesn't, it'll grab data from the internet with `gutenberg_download()` and then it will save that as `data/little_women_raw.csv`. This is really neat stuff. If you're knitting your document for the first time, you won't have the Little Women data yet, so the code will connect to Project Gutenberg and get it. The code will then save that data to your computer as a CSV file. The next time you knit, R won't need to connect to the internet again—it'll load the CSV file instead of grabbing it from Project Gutenberg. You can knit as many times as you want—you won't need to reconnect to any remote servers again.\n\nAgain, the general pattern for this is to create two chunks: (1) a fake one that people will see in the document but won't run, and (2) a real one that will run and load data locally if it exists, but that people won't see. \n\n::: {.callout-note}\n#### How should you save your intermediate data?\n\nIn the example above, I saved the Little Women data from Project Gutenberg as a CSV file. This is fine—a CSV file is plain text, so it can store any kind of text-based data like numbers and text without any problems.\n\nBut sometimes you'll work with slightly more complex types of data. For instance, with geographic data, the magical `geometry` column contains a whole host of extra metadata, like projection details and multiple points (if it's something like country boundaries). If you save a data frame with a `geometry` column as a CSV file you'll lose all that data—CSVs can't store that extra nested metadata. \n\nSimilarly, if you have a factor or categorical variable (i.e. something like \"Strongly disagree\", \"Disagree\", \"Agree\", \"Strongly agree\"), behind the scenes R stores those as numbers (i.e. 1, 2, 3, 4) with labels attached to the numbers (1 = \"Strongly disagree\", 2 = \"Disagree\", etc.). If you save a data frame with a categorical column like that as a CSV, by default R will only store the numbers and you'll lose the labels. You *could* convert the categorical column to text before saving as CSV and then the text labels would get stored, but if the variable is ordered (i.e. \"Strongly disagree\" is lower than \"disagree\", etc.), you'll lose that ordering when saving as CSV.\n\nThe safest way to save intermediate files like this is to actually not use CSV, but instead use a special kind of file called .rds, which lets you take an entire object from your Environment panel and save it as a file. The .rds file will keep all the extra metadata and attributes (i.e. the projection details and nested points inside a `geometry` column; factor labels and ordering for categorical variables, and so on).\n\nSo instead of saving that Little Women book as a CSV file, the better approach is to use `saveRDS()` and `readRDS()` to store it and load it as an .rds file, like this:\n\n````markdown\n```{{r get-book-fake, eval=FALSE}}\nlittle_women_raw <- gutenberg_download(514, meta_fields = \"title\")\n```\n\n```{{r load-book-data-real, include=FALSE}}\nlittle_women_file <- \"data/little_women_raw.rds\"\n\nif (file.exists(little_women_file)) {\n  little_women_raw <- readRDS(little_women_file)\n} else {\n  little_women_raw <- gutenberg_download(514, meta_fields = \"title\")\n  \n  saveRDS(little_women_raw, little_women_file)\n}\n```\n````\n:::\n\n::: {.callout-tip}\n#### The fancy pro version of all this\n\nIf you want to be super cool, check out [the {targets} package](https://books.ropensci.org/targets/), which is like the professional version of this approach to caching data. {targets} lets you keep track of all your different objects and it will only re-run stuff if absolutely necessary. \n\nFor instance, imagine that in a document you load data, clean it, and plot it. Standard stuff. There's a linear relationship between all this—the raw data leads to the clean data, which leads to a plot. If you change code in your plot, the data cleaning and loading code didn't change, so there's no real reason to need to re-run it. If you change your data cleaning code, your downstream plot will be affected and its code would need to be re-run.\n\n{targets} keeps track of all these dependencies and re-runs code only when there are upstream changes. It's great for plots and models that take a long time to run, or for grabbing data from the internet.\n\nThe best way to learn {targets} is to play with [their short little walkthrough tutorial here](https://books.ropensci.org/targets/walkthrough.html), which has you make a simple document that loads data, builds a regression model, and makes a plot.\n\nI use {targets} for all my projects ([including this course website!](https://github.com/andrewheiss/datavizf23.classes.andrewheiss.com#targets-pipeline)) and it makes life a ton easier for any kind of project that involves more than one .Rmd file or R script ([see this for an example](https://stats.andrewheiss.com/mountainous-mackerel/analysis/targets.html)). I *highly* recommend checking it out.\n:::\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(\n  fig.width = 6, \n  fig.height = 6 * 0.618, \n  fig.align = \"center\", \n  out.width = \"90%\",\n  collapse = TRUE\n)\n```\n\nHi everyone!\n\nJust a few quick tips from the past few weeks:\n\n\n### Can I use `geom_label_repel()` with maps?\n\nYou learned about the {ggrepel} package in [session 9](/example/09-example.qmd), with its `geom_text_repel()` and `geom_label_repel()` functions that make sure none of your labels overlap:\n\n```{r libraries-data, warning=FALSE, message=FALSE}\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(ggrepel)\n\nsmall_mpg <- mpg %>% \n  # Only use the first 10 rows\n  slice(1:10) %>% \n  # Make a label column\n  mutate(fancy_label = paste0(manufacturer, \" \", model, \" (\", year, \")\"))\n\nggplot(small_mpg, aes(x = displ, y = hwy, color = drv)) +\n  geom_point() +\n  geom_label_repel(aes(label = fancy_label), seed = 1234)\n```\n\nIn [session 12](/example/12-example.html), you learned about `geom_sf_text()` and `geom_sf_label()` for adding text and labels to maps. But what if your map labels overlap, like this?\n\n```{r counties-fake, eval=FALSE}\n# Download cb_2022_us_county_5m.zip under \"County\" from\n# https://www.census.gov/geographies/mapping-files/time-series/geo/cartographic-boundary.html\nga_counties <- read_sf(\"data/cb_2022_us_county_5m/cb_2022_us_county_5m.shp\") %>% \n  filter(STATEFP == 13)\n```\n\n```{r counties-real, include=FALSE}\nga_counties <- read_sf(here::here(\"files\", \"data\", \"external_data\", \"maps\",\n                                  \"cb_2022_us_county_5m\",\n                                  \"cb_2022_us_county_5m.shp\")) %>% \n  filter(STATEFP == 13)\n```\n\n```{r ga-places-label-overlap, warning=FALSE}\nga_places <- tribble(\n  ~city, ~lat, ~long,\n  \"Atlanta\", 33.748955, -84.388099,\n  \"Alpharetta\", 34.075318, -84.294105,\n  \"Duluth\", 34.002262, -84.143614\n) %>% \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(\"EPSG:4326\"))\n\nggplot() +\n  geom_sf(data = ga_counties, linewidth = 0.1) +\n  geom_sf(data = ga_places) +\n  geom_sf_label(data = ga_places, aes(label = city)) +\n  theme_void()\n```\n\nUnfortunately there's no such thing as `geom_sf_label_repel()`. BUT there's still a way to use `geom_label_repel()` and `geom_text_repel()` with maps, with a couple little tweaks:\n\n1. You have to map the `geometry` column in the data to the `geometry` aesthetic in `geom_text/label_repel()`\n2. You have to tell `geom_text/label_repel()` to use the \"sf_coordinates\" stat so that it uses the latitude and longitude coordinates for x/y\n\n```{r ga-places-label-fixed, warning=FALSE}\nggplot() +\n  geom_sf(data = ga_counties, linewidth = 0.1) +\n  geom_sf(data = ga_places) +\n  geom_label_repel(\n    data = ga_places,\n    aes(label = city, geometry = geometry),\n    stat = \"sf_coordinates\", seed = 1234\n  ) +\n  theme_void()\n```\n\n\n### I tried to make a map and countries are missing—why?\n\nMany of you were brave and made a map of refugee counts for mini project 2. That's fantastic!\n\nIf you did, you likely ran into an issue with plotting the countries and getting an incomplete map. Here's an example with our beloved gapminder data.\n\n```{r load-gapminder-map-data-fake, eval=FALSE}\nlibrary(countrycode)  # For dealing with country names, abbreviations, and codes\nlibrary(gapminder)    # Global health and wealth\n\n# Add an ISO country code column to gapminder for joining\ngapminder_clean <- gapminder %>% \n  mutate(ISO_A3 = countrycode(country, \"country.name\", \"iso3c\"))\n\n# Load the world map data from exercise 12\n# Download \"Admin 0 – Countries\" from\n# https://www.naturalearthdata.com/downloads/110m-cultural-vectors/\nworld_map <- read_sf(\"data/ne_110m_admin_0_countries/ne_110m_admin_0_countries.shp\") %>% \n  filter(ISO_A3 != \"ATA\") %>%  # Bye penguins\n  mutate(ISO3 = ADM0_A3)  # Use ADM0_A3 as the main country code column\n```\n\n```{r load-gapminder-map-real, include=FALSE}\nlibrary(countrycode)\nlibrary(gapminder)\n\ngapminder_clean <- gapminder %>% \n  mutate(ISO3 = countrycode(country, \"country.name\", \"iso3c\"))\n\nworld_map <- read_sf(here::here(\"files\", \"data\", \"external_data\", \"maps\",\n                                \"ne_110m_admin_0_countries\", \n                                \"ne_110m_admin_0_countries.shp\")) %>% \n  filter(ISO_A3 != \"ATA\") %>% \n  mutate(ISO3 = ADM0_A3)\n```\n\nLet's take just 2007 from gapminder and map life expectancy. To do this we'll need to combine or join the two datasets. One logical way to do this would be to take gapminder, join the world map data to it, and then plot it:\n\n```{r gapminder-2007-error, error=TRUE}\ngapminder_with_map <- gapminder_clean %>% \n  filter(year == 2007) %>% \n  left_join(world_map, by = join_by(ISO3))\n\nggplot() +\n  geom_sf(data = gapminder_with_map, aes(fill = lifeExp))\n```\n\noh no there's an error! When we joined the map data, the special attributes of the `geometry` column in `world_map` got lost. The column is still there, but it won't automatically plot with `geom_sf()`. We can fix that by specifying that the column named \"geometry\" does indeed contain all the geographic data with `st_set_geometry()`:\n\n```{r gapminder-2007-missing}\ngapminder_with_map <- gapminder_clean %>% \n  filter(year == 2007) %>% \n  left_join(world_map, by = join_by(ISO3)) %>% \n  # Fix the geometry column\n  st_set_geometry(\"geometry\")\n\nggplot() +\n  geom_sf(data = gapminder_with_map, aes(fill = lifeExp)) +\n  theme_void()\n```\n\nWe have a… map? It's missing a bunch of countries (Russia is the most glaringly obvious hole!). That's because those countries aren't in gapminder, so their corresponding maps didn't come over when using `left_join()`. We can confirm by counting rows. The original map data has maps for 176 countries. Gapminder has 142 countries in 2007. The combined `gapminder_with_map` dataset only has 142 rows—we're not plotting 34 countries, since they're not in gapminder.\n\n```{r count-rows}\nnrow(world_map)\nnrow(gapminder_clean %>% filter(year == 2007))\nnrow(gapminder_with_map)\n```\n\nOne quick and easy way to fix this is to use two `geom_sf()` layers: one with the whole world and one with the partial gapminder-only map:\n\n```{r two-geom-sf-layers}\nggplot() +\n  geom_sf(data = world_map) +\n  geom_sf(data = gapminder_with_map, aes(fill = lifeExp)) +\n  theme_void()\n```\n\nThe *better* way to fix this is to join the two datasets in a different order—start with the full map data and then add gapminder to it. This maintains the specialness of the geometry column and keeps all the original rows in `world_map`. For countries that are in the map data but not in gapminder, they'll still be in the final `map_with_gapminder` data, but they'll have NA for life expectancy:\n\n```{r gapminder-2007-good}\nmap_with_gapminder <- world_map %>% \n  left_join(filter(gapminder_clean, year == 2007), by = join_by(ISO3))\n\nggplot() +\n  geom_sf(data = map_with_gapminder, aes(fill = lifeExp)) +\n  theme_void() +\n  # Make the countries with missing data a different color\n  scale_fill_gradient(na.value = \"grey90\")\n```\n\nWhat if we want to facet though? This is just one year—what if we want to show panels for multiple years? This gets a little tricky. The gapminder data has rows for different country/year combinations (Afghanistan 1952, Afghanistan 1957, Albania 1952, etc.), but the world map data only has rows for countries. If we join the gapminder data to the world map data and gapminder has multiple rows for years, there's no clear place for the gapminder rows to connect with the world map rows. R will try to make it work and repeat world_map rows for each of the repeated years, but it can be unpredictable.\n\nThe best approach I've found for doing this is to create what I call a \"skeleton\" data frame that has all the possible combinations of (1) unique countries in the map data and (2) unique years in gapminder (or the refugee data if you're using that). The `expand_grid()` function does this automatically. Like, look what happens if we tell it to make rows for every combination of A, B, C and 1, 2, 3—we get A1, A2, A3, B1, B2, and so on:\n\n```{r expand-grid-example}\nexpand_grid(column_1 = c(\"A\", \"B\", \"C\"),\n            column_2 = c(1, 2, 3))\n```\n\nWe'll make a similar skeleton with all the countries in the map and all the years we care about in gapminder. We'll just show two panels—1952 and 2007—so we'll make a little filtered dataset first. Then we'll use `expand_grid()` to make a dataset with all those combinations: Afghanistan 1952, Afghanistan 2007, Albania 1952, Albania 2007, and so on:\n\n```{r create-initial-skeleton}\ngapminder_smaller <- gapminder_clean %>%\n  filter(year %in% c(1952, 2007))\n\nskeleton <- expand_grid(ISO3 = unique(world_map$ISO3),\n                        year = unique(gapminder_smaller$year))\nskeleton\n```\n\nNeat, that works. There's Fiji in 1952 and 2007, Tanzania in 1952 and 2007, and so on. Those are all the possible countries in `world_map` with all the possible years in `gapminder_smaller`.\n\nNext we can join in the gapminder data for each country and year, and join in the map data for each country. Notice how it has the same number of rows as `skeleton` (352). If a country doesn't have gapminder data (like Fiji here), it gets an NA for `lifeExp` and `pop` and `gdpPercap`. But it still has map data for both 1952 and 2007, so it'll show up in a plot.\n\n```{r make-full-gapminder-data}\nfull_gapminder_map <- skeleton %>% \n  left_join(gapminder_smaller, by = join_by(ISO3, year)) %>%\n  left_join(world_map, by = join_by(ISO3)) %>% \n  # The geometry column lost its magic powers after joining, so add it back\n  st_set_geometry(\"geometry\")\nfull_gapminder_map\n```\n\nNow we can plot it and we'll have consistent countries in each panel:\n\n```{r gapminder-map-fixed}\nggplot() +\n  geom_sf(data = full_gapminder_map, aes(fill = lifeExp)) +\n  facet_wrap(vars(year), ncol = 1) +\n  scale_fill_gradient(na.value = \"grey90\") +\n  theme_void()\n```\n\nPerfect!\n\n\n### Some of the words in my word frequency/tf-idf plot were out of order—how can I fix that?\n\nIn the [example for week 13](/example/13-example.html#tokens-and-word-counts), I showed the 15 most frequent words in Hamlet, Macbeth, Romeo and Juliet, and King Lear, faceted by play. Only Romeo and Juliet, though, has the words in the correct order. The other plays have strange ordering. Note how \"lord\" and \"king\" are weirdly misplaced in Macbeth and Hamlet and how \"love\" is weirdly misplaced in Hamlet:\n\n![Words in the wrong order across panels](img/plot-top-words-tragedies-annotated.png)\n\nThe word \"lord\" is the second most common word in Hamlet, so R thinks it is the second most common word across all the plays. It doesn't know that there's a difference between \"lord\" in Hamlet and \"lord\" in Macbeth. As a result, any common words that are shared across the plays will appear out of order.\n\nThis is fixable though! [See this blog post by Julia Silge](https://juliasilge.com/blog/reorder-within/), one of the authors of {tidytext}. Basically, you need to use `reorder_within()` to sort the words correctly inside each play, then add `scale_y_reordered()` to make them display correctly.\n\nHere's what that looks like with the Shakespeare words.\n\n```{r libraries-data-reordering, warning=FALSE, message=FALSE}\nlibrary(tidyverse)\nlibrary(gutenbergr)  # For getting books from Project Gutenberg\nlibrary(tidytext)    # For working with text\n```\n\n```{r get-text-fake, eval=FALSE}\ntragedies_raw <- gutenberg_download(\n  c(\n    1524,  # Hamlet\n    1532,  # King Lear\n    1533,  # Macbeth\n    1513   # Romeo and Juliet\n  ),\n  meta_fields = \"title\"\n)\n```\n\n```{r load-saved-text, include=FALSE}\nwithr::with_dir(here::here(), {\n  invisible(list2env(targets::tar_read(gutenberg_books), .GlobalEnv))\n})\n```\n\n```{r clean-tragedies, message=FALSE}\n# Clean up the tragedies text\ntop_words_tragedies <- tragedies_raw %>% \n  drop_na(text) %>% \n  unnest_tokens(word, text) %>% \n  # Remove stop words\n  anti_join(stop_words) %>% \n  # Get rid of old timey words and stage directions\n  filter(!(word %in% c(\"thou\", \"thy\", \"haue\", \"thee\", \n                      \"thine\", \"enter\", \"exeunt\", \"exit\"))) %>% \n  # Count all the words in each play\n  count(title, word, sort = TRUE) %>% \n  # Keep top 15 in each play\n  group_by(title) %>% \n  top_n(15) %>% \n  ungroup()\ntop_words_tragedies\n```\n\nBecause we used `top_n()`, these words are already sorted in order of frequency (with \"hamlet\" appearing the most at 461 times). In example 13, we locked in that order by making the `word` column an ordered factor, like this:\n\n```{r plot-top-words-tragedies-wrong}\ntop_words_tragedies_order_wrong <- top_words_tragedies %>%\n  # Make the words an ordered factor so they plot in order\n  mutate(word = fct_inorder(word)) \n\nggplot(top_words_tragedies_order_wrong, aes(y = fct_rev(word), x = n, fill = title)) + \n  geom_col() + \n  guides(fill = \"none\") +\n  labs(y = \"Count\", x = NULL, \n       title = \"15 most frequent words in four Shakespearean tragedies\") +\n  facet_wrap(vars(title), scales = \"free_y\") +\n  theme_bw()\n```\n\nBut that's wrong!\n\nInstead of using `fct_inorder()`, we need to use `reorder_within()` and tell it to sort the words by count within each play:\n\n```{r tragedies-reorder-within}\ntop_words_tragedies_order_right <- top_words_tragedies %>%\n  # Make the words an ordered factor so they plot in order\n  mutate(word = reorder_within(word, n, title)) \n\ntop_words_tragedies_order_right\n```\n\nNotice how the `word` column looks a little weird now. It added the play name to the end of each word, like `macbeth___Macbeth`. That's actually a creative hack for fixing the ordering. Remember that the main reason the ordering is messed up across facets is because R doesn't know that the word \"love\" in Hamlet is different from the word \"love\" in Romeo and Juliet. By changing the words to `love___Romeo and Juliet` and `love___Hamlet`, R can now recognize the different versions of \"love\" and sort them correctly. Let's plot this version:\n\n```{r plot-top-words-tragedies-right-but-wrong}\nggplot(top_words_tragedies_order_right, aes(y = word, x = n, fill = title)) + \n  geom_col() + \n  guides(fill = \"none\") +\n  labs(y = \"Count\", x = NULL, \n       title = \"15 most frequent words in four Shakespearean tragedies\") +\n  facet_wrap(vars(title), scales = \"free_y\") +\n  theme_bw()\n```\n\noh no.\n\nThe order is right (yay!) but the y-axis is horrible since it's including the hacky `___play name` at the end of each of the words. \n\nTo fix that, we can use `scale_y_reordered()`, which cleans up those word labels by removing the three underscores and any text that follows them:\n\n```{r plot-top-words-tragedies-right}\nggplot(top_words_tragedies_order_right, aes(y = word, x = n, fill = title)) + \n  geom_col() + \n  scale_y_reordered() +\n  guides(fill = \"none\") +\n  labs(y = \"Count\", x = NULL, \n       title = \"15 most frequent words in four Shakespearean tragedies\") +\n  facet_wrap(vars(title), scales = \"free_y\") +\n  theme_bw()\n```\n\nPerfect!\n\n\n### Cleaning up text is *always* specific and specialized\n\nIn the Shakespeare example, we removed common stop words like \"the\" and \"a\" with `anti_join()` and then manually removed some other more specific words like \"thou\" and \"thee\" and \"exit\":\n\n```{r old-timey-words, eval=FALSE}\n# Clean up the tragedies text\ntop_words_tragedies <- tragedies_raw %>% \n  drop_na(text) %>% \n  unnest_tokens(word, text) %>% \n  # Remove stop words\n  anti_join(stop_words) %>% \n  # Get rid of old timey words and stage directions\n  filter(!(word %in% c(\"thou\", \"thy\", \"haue\", \"thee\", \n                       \"thine\", \"enter\", \"exeunt\", \"exit\")))\n```\n\nThat's because in these specific plays, those are common words that we want to ignore—they're basically our own custom stop words. We should also probably get rid of words like \"act\" and \"scene\" too, but we didn't here.\n\nMany of you kept that exact code in exercise 13, removing \"thou\", \"thy\", \"exeunt\", and those other words from your own text. But that's not necessary or helpful. If you're working with something like Harry Potter or Jane Austen or Ernest Hemmingway or anything more modern than Shakespeare, those words aren't really in there. In the Shakespeare example, we removed \"enter\" and \"exit\" because those are stage directions, but in other books those are regular actual words and probably shouldn't be removed.\n\nThere's no one universal set of stop words that you can use—every text is unique and has its own quirks that you need to take care of. \n\nFor example, one of you looked at four books by W. E. B. Du Bois and did this to clean up the stop words:\n\n```{r dubois-words, eval=FALSE}\ndubois_clean %>%\n  anti_join(stop_words) %>% \n  filter(!(word %in% c(\"1\", \"2\", \"cong\", \"sess\", \"act\", \"pp\", \"_ibid\",\n                       \"_house\", \"3\", \"doc\")))\n```\n\nThat's awesome. Those are all words that are specific to those four books and that were likely appearing in the frequency plot. One (or more) of the books probably mentioned lots of congressional activity, like congressional sessions, acts of congress, stuff happening in the House of Representatives, and so on. There were probably also a lot of citations, with things like \"pp.\" (the abbreviation for \"pages\", like \"pp. 125-127\") and \"ibid\" (the abbreviation for \"see the previous citation\"). That list of words is specific to those four books and *should not* be applied to other books—like, there's no reason to remove those words from the Shakespeare tragedies or from Little Women or from Harry Potter because none of those mention congressional sessions or use \"ibid\".\n\nData cleaning is *always* context specific.\n\n\n### Saving data that takes a long time to make\n\nIn these later sessions, I've had you do things with data from different places on the internet. In [exercise 13](/example/13-example.qmd) you grabbed books from Project Gutenberg. Some of you used [`osrmRoute()`](https://www.andrewheiss.com/blog/2023/06/01/geocoding-routing-openstreetmap-r/#routing) in exercise 12 to create a mapped route between cities. Some of you used [{tidygeocoder}](/example/12-example.qmd#automatic-geocoding-by-address) to geocode addresses in exercise 12. In past sessions you've used `WDI()` to download data from the World Bank.\n\nWhen you knit a document, R starts with a brand new empty session without any packages or data loaded, and then it runs all your chunks to load packages, load data, and run all your other code. If you have code that grabs data from the internet, **it will run every time you knit your document**. [Remember my suggestion to knit often](/news/2023-10-03_cleaner-nicer-rmd-output.qmd#knit-often)? You'll re-download the data, re-create routes, re-geocode addresses, and so on every time you keep re-knitting. This is excessive, slow, and—most especially—bad R etiquette. You don't want to keep accessing those servers and recalculate things and redownload things you don't need to update. \n\nBUT at the same time, you should care about reproducibility. You want others—and future you—to be able to run your code and create the same plots and tables and get the same data. But you don't want to do all that excessively and impolitely.\n\nThe solution is to be a little tricky with your R Markdown file. If you have code that needs to grab something from the internet, put it in a chunk that doesn't run—use `eval=FALSE` in its chunk options. Then, in an invisible chunk (with `include=FALSE`) load the pre-downloaded data manually. I showed this in [example 8](/example/08-example.qmd#load-and-clean-data) (with {WDI}) and [example 11](/example/11-example.qmd#get-data) (with {tidyquant}) and [example 13](/example/13-example.qmd#get-data) (with {gutenberger})\n\nHere's a quick basic example with Project Gutenberg book data. There are two chunks: `get-book-fake` and `load-book-real`: \n\n````markdown\n```{{r get-book-fake, eval=FALSE}}\nlittle_women_raw <- gutenberg_download(514, meta_fields = \"title\")\n```\n\n```{{r load-book-data-real, include=FALSE}}\nlittle_women_file <- \"data/little_women_raw.csv\"\n\nif (file.exists(little_women_file)) {\n  little_women_raw <- read_csv(little_women_file)\n} else {\n  little_women_raw <- gutenberg_download(514, meta_fields = \"title\")\n  \n  write_csv(little_women_raw, little_women_file)\n}\n```\n````\n\n1. The first chunk (`get-book-fake`) contains the code for downloading data with `gutenberg_download()`. It will appear in the document, but **it will not run** since it has `eval=FALSE` on. It will not try to grab anything from the internet. If someone were to follow along with the code in your document, they could run that code and get the book (good!), and it won't run repeatedly on your end (also good!).\n2. The second chunk (`load-book-data-real`) does a neat little trick. It first checks to see if a CSV file named `data/little_women_raw.csv` exists. If it does, it'll just load it with `read_csv()`. If it doesn't, it'll grab data from the internet with `gutenberg_download()` and then it will save that as `data/little_women_raw.csv`. This is really neat stuff. If you're knitting your document for the first time, you won't have the Little Women data yet, so the code will connect to Project Gutenberg and get it. The code will then save that data to your computer as a CSV file. The next time you knit, R won't need to connect to the internet again—it'll load the CSV file instead of grabbing it from Project Gutenberg. You can knit as many times as you want—you won't need to reconnect to any remote servers again.\n\nAgain, the general pattern for this is to create two chunks: (1) a fake one that people will see in the document but won't run, and (2) a real one that will run and load data locally if it exists, but that people won't see. \n\n::: {.callout-note}\n#### How should you save your intermediate data?\n\nIn the example above, I saved the Little Women data from Project Gutenberg as a CSV file. This is fine—a CSV file is plain text, so it can store any kind of text-based data like numbers and text without any problems.\n\nBut sometimes you'll work with slightly more complex types of data. For instance, with geographic data, the magical `geometry` column contains a whole host of extra metadata, like projection details and multiple points (if it's something like country boundaries). If you save a data frame with a `geometry` column as a CSV file you'll lose all that data—CSVs can't store that extra nested metadata. \n\nSimilarly, if you have a factor or categorical variable (i.e. something like \"Strongly disagree\", \"Disagree\", \"Agree\", \"Strongly agree\"), behind the scenes R stores those as numbers (i.e. 1, 2, 3, 4) with labels attached to the numbers (1 = \"Strongly disagree\", 2 = \"Disagree\", etc.). If you save a data frame with a categorical column like that as a CSV, by default R will only store the numbers and you'll lose the labels. You *could* convert the categorical column to text before saving as CSV and then the text labels would get stored, but if the variable is ordered (i.e. \"Strongly disagree\" is lower than \"disagree\", etc.), you'll lose that ordering when saving as CSV.\n\nThe safest way to save intermediate files like this is to actually not use CSV, but instead use a special kind of file called .rds, which lets you take an entire object from your Environment panel and save it as a file. The .rds file will keep all the extra metadata and attributes (i.e. the projection details and nested points inside a `geometry` column; factor labels and ordering for categorical variables, and so on).\n\nSo instead of saving that Little Women book as a CSV file, the better approach is to use `saveRDS()` and `readRDS()` to store it and load it as an .rds file, like this:\n\n````markdown\n```{{r get-book-fake, eval=FALSE}}\nlittle_women_raw <- gutenberg_download(514, meta_fields = \"title\")\n```\n\n```{{r load-book-data-real, include=FALSE}}\nlittle_women_file <- \"data/little_women_raw.rds\"\n\nif (file.exists(little_women_file)) {\n  little_women_raw <- readRDS(little_women_file)\n} else {\n  little_women_raw <- gutenberg_download(514, meta_fields = \"title\")\n  \n  saveRDS(little_women_raw, little_women_file)\n}\n```\n````\n:::\n\n::: {.callout-tip}\n#### The fancy pro version of all this\n\nIf you want to be super cool, check out [the {targets} package](https://books.ropensci.org/targets/), which is like the professional version of this approach to caching data. {targets} lets you keep track of all your different objects and it will only re-run stuff if absolutely necessary. \n\nFor instance, imagine that in a document you load data, clean it, and plot it. Standard stuff. There's a linear relationship between all this—the raw data leads to the clean data, which leads to a plot. If you change code in your plot, the data cleaning and loading code didn't change, so there's no real reason to need to re-run it. If you change your data cleaning code, your downstream plot will be affected and its code would need to be re-run.\n\n{targets} keeps track of all these dependencies and re-runs code only when there are upstream changes. It's great for plots and models that take a long time to run, or for grabbing data from the internet.\n\nThe best way to learn {targets} is to play with [their short little walkthrough tutorial here](https://books.ropensci.org/targets/walkthrough.html), which has you make a simple document that loads data, builds a regression model, and makes a plot.\n\nI use {targets} for all my projects ([including this course website!](https://github.com/andrewheiss/datavizf23.classes.andrewheiss.com#targets-pipeline)) and it makes life a ton easier for any kind of project that involves more than one .Rmd file or R script ([see this for an example](https://stats.andrewheiss.com/mountainous-mackerel/analysis/targets.html)). I *highly* recommend checking it out.\n:::\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"highlight-style":"monokai","toc-depth":4,"output-file":"2023-11-29_faqs_week-11-12-13.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Posted","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","bibliography":["../files/bib/readings.bib"],"csl":"../files/bib/chicago-author-date.csl","_quarto-vars":{"author":"Andrew Heiss","instructor":{"name":"Dr. Andrew Heiss","name_no_title":"Andrew Heiss","email":"aheiss@gsu.edu","url":"https://www.andrewheiss.com","twitter":"andrewheiss","github":"andrewheiss","office":"55 Park Place SE, Room 464","contact_policy":"E-mail and Slack are the best ways to get in contact with me. I will try to respond to all course-related e-mails and Slack messages within 24 hours (*really*), but also remember that life can be busy and chaotic for everyone (including me!), so if I don't respond right away, don't worry!","appointment_url":"https://calendly.com/andrewheiss/"},"course":{"number":"PMAP 8551/4551","semester":"Fall 2023","days":"Any day","time":"Asynchronous","location":"Anywhere","dates":"August 12–December 11, 2023","github":"https://www.github.com/andrewheiss/datavizf23.classes.andrewheiss.com","url":"https://datavizf23.classes.andrewheiss.com","copyright_year":"2023","slack":"https://gsudatavizf2023.slack.com"},"university":{"name":"Georgia State University","url":"https://www.gsu.edu"},"school":{"name":"Andrew Young School of Policy Studies","url":"https://aysps.gsu.edu/"}},"theme":["litera","../html/custom.scss"],"date-format":"dddd MMMM D, YYYY [at] h:mm A","template-partials":["../html/news/title-block.html"],"title":"Weeks 11, 12, and 13 tips and FAQs","date":"2023-11-29T14:34","categories":["FAQs"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}